\documentclass{article}
\usepackage{natbib}
\usepackage{color}
\usepackage[pdftex, pdfborderstyle={/S/U/W 0}]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{proof}
\usepackage{listings}
\lstloadlanguages{ContextualML}
\lstset{language=ContextualML}  % Added to local lstlang2.sty

\renewcommand{\t}[1]{{\fontsize{1.0\zzlistingsize}{1.05\zzlistingsize}\texttt{#1}}}
% Set up listings "literate" keyword stuff (for \lstset below)
\newdimen\zzlistingsize
\newdimen\zzlistingsizedefault
\zzlistingsizedefault=9pt
\zzlistingsize=\zzlistingsizedefault
\global\def\InsideComment{0}
\newcommand{\Lstbasicstyle}{\fontsize{\zzlistingsize}{1.05\zzlistingsize}\ttfamily}
\newcommand{\keywordFmt}{\fontsize{0.9\zzlistingsize}{1.0\zzlistingsize}\bf}
\newcommand{\smartkeywordFmt}{\if0\InsideComment\keywordFmt\fi}
\newcommand{\commentFmt}{\def\InsideComment{1}\fontsize{0.95\zzlistingsize}{1.0\zzlistingsize}\rmfamily\slshape}

\newcommand{\LST}{\setlistingsize{\zzlistingsizedefault}}

\newlength{\zzlstwidth}
\newcommand{\setlistingsize}[1]{\zzlistingsize=#1%
\settowidth{\zzlstwidth}{{\Lstbasicstyle~}}}
\setlistingsize{\zzlistingsizedefault}
\lstset{literate={->}{{$\rightarrow~$}}2 %
                 {=>}{{$\Rightarrow~$}}2 %
                 {id}{{{\smartkeywordFmt id}}}1 % 3 $~$
                 {\\}{{$\lambda$}}1 %
                 {\\Pi}{{$\Pi$}}1 %
                 {\\Sigma}{{$\Sigma$}}1 %
                 {\\Pibox}{{$\Pibox$}}1 %
                 {\\psi}{{$\psi$}}1 %
                 {\\phi}{{$\phi$}}1 %
                 {\\sigma}{{$\sigma$}}1 %
                 {\\gamma}{{$\gamma$}}1 %
%                 {mlam}{{$\lambda^{\scriptscriptstyle\Box}$}}1 %
                 {mlam}{{$\lambda$}}1 %
                 {FN}{{$\Lambda$}}1 %
                 {<<}{\color{dGreen}}1 %
                 {>>}{\color{black}}1 %
                 {^}{{$\cdot$}}1 %
               ,
               columns=[l]fullflexible,
               basewidth=\zzlstwidth,
               basicstyle=\Lstbasicstyle,
               keywordstyle=\keywordFmt,
               identifierstyle=\relax,
%               stringstyle=\relax,
               commentstyle=\commentFmt,
               breaklines=true,
               breakatwhitespace=true,   % doesn't do anything (?!)
               mathescape=true,   % interprets $...$ in listing as math mode
%               tabsize=8,
               texcl=false}

\input{notation}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\author{Andrew Cave}
\title{Dependent type theory for contextual reasoning}

\definecolor{light-gray}{gray}{0.5}
\newcommand{\LONGVERSION}[1]{{\color{light-gray}#1}}

\begin{document}
\maketitle

\section{Introduction}

%(todo: look at other examples of thesis statements)

Increasingly, there is a desire to write computer checked proofs for
research in programming languages and logic
\citep{POPLMark}. Results in programming languages appear to be prime
candidates for computer checked proofs. For one, these proofs typically take the
form of a proof by induction with many cases, and formalizing inductive
reasoning is by now quite well understood. Moreover, the abundance of cases means that such proofs can be
tedious and error-prone to write and to verify by hand. Fortunately, it is now commonplace for proof
assistants to not only check that all cases have been covered, but
even to automatically generate the list of cases for the human prover
to consider, so that constructing a proof becomes an interactive
effort.

\LONGVERSION{(proof carrying code? want to formalize your ``domain specific'' reasoning logic and mechanize
its metatheory)}

However, formalizing such proofs can still be tedious for at least one reason,
namely giving a formal treatment of variable binding and
substitution. \cite{Altenkirch93} writes (about
formalizing a proof of normalization for System F):

\begin{quote}
When doing the formalization, I discovered that the core part of the
proof (here proving the lemmas about CR) is fairly straightforward and
only requires a good understanding of the paper version. However, in
completing the proof I observed that in certain places I had to invest
much more work than expected, e.g. proving lemmas about substitution
and weakening.
\end{quote}

This issue has spawned a large body of work aiming to relieve this
particular burden. Nominal logic \cite{?} supports names and weakening
effectively, but is relatively silent about substitution. The
logical framework LF \citep{Harper93jacm} has proven to be well-suited
for representing programming language syntax, logics and hypothetical
judgments and it provides direct support for substitution using an
idea called higher-order abstract syntax (HOAS): bindings are
represented using LF's \emph{function space}. While higher-order
abstract syntax has proven to be one of the most 
effective approaches to the problem of binding and substitution, it
has come at a fairly steep price: most theories and systems
incorporating higher order abstract syntax have fairly weak
logics. This is in part because traditionally, it was unclear how to
reconcile the traditional notions of implication and function spaces with
the ``weak'' function space used in HOAS as a purely \emph{representational} technique.

For example, Twelf \cite{Twelf} can only express $\Pi_2$
statements, i.e. statements of the form $\forall x \exists y. \psi$
where $\psi$ contains no quantifiers. Beluga (?), Abella (?), and
Delphin (?) are
somewhat better: they are all, roughly speaking, first order logic
with induction. These weak logics suffice for many results in
programming language metatheory, such as type soundness, but it is
difficult, if not outright impossible, to express more sophisticated
results such as normalization and parametricity, or generally to build
interesting models. If we wish to mechanize these (and other
state-of-the-art) results, we need foundations which lie closer in
strength to foundations of mathematics, and we want as much support as
possible for variable binding and substitution.

There are a handful of approaches to logics and theories rich enough
to serve as foundations for mathematics. Higher order logic, set theory, and dependent
type theory are the main alternatives. Among these, dependent type theory
\citep{Martin-Loef73a} boasts the advantage that it comes equipped with a direct
connection to computer programming. This means that it provides a rich
environment which at the same time supports both proofs and
programs. By constructing a proof in dependent type theory, one has
implicitly also constructed a corresponding algorithm which can be
directly executed. This is very valuable, because it
enables the development of theory to proceed hand in hand
with implementation. Moreover, (intensional) dependent type theory
incorporates automated support for a decidable equational theory in a
principled way, which allows fully formal proofs to be remarkably
short (at the cost of a more expensive proof-checking
procedure). This is an important asset for the proposed work. 

\LONGVERSION{add citations to NuPRL \citep{NuPRL} Agda \citep{Norell:phd07} Coq
\citep{bertot/casteran:2004} somewhere}

The main claim I aim to support with my thesis work is the following:

\begin{quote}Building a logical framework into dependent type
theory is possible and makes mechanization of state-of-the-art programming
language metatheory more effective.
\end{quote}

Moreover: the resulting framework shows promise for a rich programming
environment in which to implement programming languages and logics, which allows
implementation to be carried out jointly with a ``sliding-scale'' of
verification.

More generally, I hope that the lessons learned from these efforts can shed some light on how
to make mechanization of proofs outside of programming languages and
logic more practical. A main component of the proposed work is the
incorporation of a (particular) rich, decidable equational theory into a
proof assistant, and this effort seems very likely to offer insight into how to
incorporate other equational theories from other areas of application.


\section{Background}
To make the issues involved more concrete, and later to give a sketch
of proposed work, I briefly describe the
well-known issues involved in dealing with a first-order representation of
syntax. Below I sketch an intrinsically typed encoding of the
simply typed lambda calculus in a simplified Agda-like notation. This
is a well-known technique \cite{Benton?Coqencodingspaper} for
economically describing well-typed expressions, and enforcing some
invariants about them in a lightweight way.

\begin{verbatim}
data exp (G : ctx) : tp -> Set where
 v : var G T -> exp G T
 app : exp G (arr T S) -> exp G T -> exp G S
 lam : exp (G , T) S -> exp G (arr T S)
\end{verbatim}

Here, \lstinline{exp} is a family of types indexed by  a context
\lstinline{G} (which we can think of a list of types) and its
type. We have three constructors, \lstinline{v}, \lstinline{app}, and
\lstinline{lam}. Variables are represented as de Bruijn indices and
embedded into terms with \lstinline{v}, but we do
not go into detail about this here. When describing an operational
semantics, we quickly have need to describe substitution of
expressions for variables. This in itself is not so difficult,
although it is somewhat repetitive. We can
describe (simultaneous) substitutions \lstinline{sub G' G} which provide
instantations for the variables in \lstinline{G} as expressions in
\lstinline{G'}, and write a function which applies a substitution to
an expression. We may also write a function which composes two
substitutions. We also need functions for weakening terms
\lstinline{exp G T} to \lstinline{exp (G , S)$\,$ T}, which are equally
burdensome, but which I do not discuss below.

\begin{alltt}
[_] : sub G' G -> exp G T -> exp G' T = ...
[_]' : sub G' G -> sub G D -> sub G' D = ...
\end{alltt}

This is sufficient in this case to describe the operational semantics
of our language, but if we wish to prove results about the operational
semantics (for example, confluence), we quickly find that we need to
prove some equations about the application of substitutions. Typical
results we may need are listed below:

\begin{alltt}
lem1 : [ [ \(\sigma1\) ]' \(\sigma2\) ] = [ \(\sigma1\) ] \(\circ\) [ \(\sigma2\) ]
lem2 : [ \(\sigma\) , M ] \(\circ\) \(\uparrow\) = [ \(\sigma\) ]
lem3 : [ id ]' \(\sigma\) = \(\sigma\)
lem4 : [ id , N ] \(\circ\) [ \(\uparrow\) \(\sigma\), 1 ] = [ \(\sigma\) , N ]
\end{alltt}

For example, \lstinline{lem1} says that the composition of applying
substitutions is the application of the composition of
substitutions. I have only listed four of these lemmas, and only the
ones pertaining to substitution, not to weakening. In all, one
typically finds that it is necessary to prove about a dozen such lemmas. 

One begins to lament: all this effort for one of the simplest imaginable programming
languages? As the number of constructs in our language grows, or as we
begin to consider other languages, these lemmas quickly become a major
burden. These problems can not be blamed on de Bruijn indices: if we were to use a named representation of variables, these
lemmas simply take different forms, but do not really become any
simpler.

Higher-order abstract syntax solves many of these issues. Roughly the
idea is to directly embed the language of study inside of LF:
constructors become constants, bindings become *functions*. Difficult
part is recovering a view of this HOAS data as a first-order
representation: we want a form of induction \emph{over} a HOAS representation.
 It does not support simultaneous substitutions, only
individual substitutions $[N/x]M$. Individual substitutions suffice
for many purposes, but some problems are much more naturally handled
with simultaneous substitutions. And simultaneous substitutions have a
larger equational theory that needs to be treated. This is something
that LF remains silent about: we need to do much of the same effort
for simultaneous substitution as we did in the first-order
representation. In my view, Beluga, Abella, Delphin are approaches to
recovering a ``first-order view'' of ``higher-order representations''.

Beluga, Abella, and Delphin maintain a strict
separation between the two levels. This means it is not so clear how
to support the automated equational theory that makes intensional type
theory so powerful.

Contrast dependent type theory with indexed types (e.g. Beluga) and first-order
logic (e.g. Abella). 

clarify distinction with Hybrid: I'm proposing to use language design
techniques to give better support (definitional equalities for
simultaneous substitutions).

Citations? parametricity \citep{Reynolds,morerecent?}, decidability of
  typechecking \citep{Coquand?}, (step-indexed) logical relations for contextual
  equivalence \citep{Dreyer11,others}, soundness and completenes of
  NbE \citep{Dybjer00,Abel07}, all of which require
  richer logics to express. Sometimes also simple requirements like
  concatenation of contexts -- can't directly be expressed in Twelf
  (can in Beluga), can often be worked around, but why not directly? 
  By giving a more seamless integration of a logical framework
  into dependent type theory (i.e. no separation between levels) this
  can be accomplished.

Also forgetful maps on contexts (v.s. representation using relations
and proving totality, etc)

Also to properly justify logical relations arguments and use existing
understanding of termination. (We ``cheat'' in Beluga to do logical relations)

(Can I claim that the PL community is one of the biggest users of
proof assistants, hence it makes sense to target?)

Pro of de Bruijn indices and contextual types (vs names, nominal
logic): purely equational; no ``propositional'' side
conditions. Compare:

$[N/x]M = M$ if $x \not\in fv(M)$, versus:

$[0 := N]M = M$ if $M = [\uparrow]M'$ for some $M'$.

This fits better into dependent type theory.

HOAS/contextual types are ``syntax'' on top of de Bruijn indices.

cite structural logical relations: requires auxiliary logic, deviates
somewhat from the on-paper techniques? \citep{Schurmann08}

In defense of intensional type theory: *smaller* checkable
witnesses. (both beta and OL substitution equations)

Spells out equality testing -- algorithm, not heuristic.

\subsection{Conversion in type theory}
A central issue in the design of a dependent type theory is
conversion: when are two types deemed to be equal? In
intensional type theories, conversion is decidable, but somewhat
weak, typically including only $\beta$ laws and perhaps some $\eta$
laws. There has been substantial work on the design and verification
of algorithms for deciding increasingly richer theories, typically
with more forms of $\eta$ laws. To take a
small selection: \cite{Coquand91,Harper05,Abel11}. Recently,
\cite{Allais13} have developed promising techniques for incorporating equations
which are neither $\beta$ nor $\eta$ laws, such as the associativity
and unit laws for list append. It remains open to establish if 
these techniques can be applied to rich dependent type theories.

\section{Summary of progress to date}
I have published two papers in this direction to date. The first
was presented at POPL'12 \citep{Cave12} , and was a first step in
exploring how HOAS can fit into a rich type theory, targetting support
for indexed recursive types. The second was presented at LFMTP'13
\citep{Cave13}, and addresses direct support for the notion of
simultaneous substitution, a concept which has been absent from most
work on HOAS, but which is nevertheless extremely useful. I am
preparing a third paper \citep{Cave14} for submission to the Journal
of Formalized Reasoning, which explains how the tools described the
previous two papers can be successfully applied in the formalization
of a substantial logical relations proof. In this
section I briefly explain the main contributions of these papers.

\subsection{Indexed recursive types}
The POPL'12 paper develops a notion of LF-indexed recursive types
\citep{Cave12}, and describes many useful applications. This was a
first step in exploring how HOAS types can fit
into a rich type theory. This work emphasized ``programming'' applications
as opposed to reasoning, since it was an open problem at the time how
to support termination analysis in this setting, which is necessary to
verify that a recursive function corresponds to a genuine inductive proof. However, the techniques
developed here also improved understanding of how to \emph{reason} about
HOAS encodings, even if such proofs are yet not fully formal.

Indexed recursive types significantly increase the expressive power of a system: one can express
predicates on LF objects which were inexpressible before: e.g. that
certain subexpressions are closed (they do not refer to any free variables), and express relationships between
contexts. Generally, it gives direct access to properties of
contexts. This is similar to Abella, but in contrast to Delphin, and Twelf which do not
provide such direct access to the context.

Recursive types are suitable for programming, but they do not suffice
if we are interested in proofs: need positivity. don't quite suffice
for logical relations: need universe (large eliminations) to justify,
since they aren't positive definitions.

In this paper, we described an intrinsically type-preseriving
normalization by evaluation algorithm. This algorithm is traditionally
difficult or impossible for systems based on HOAS, because it requires
a comparatively rich computation/reasoning layer in order to define a semantic domain for expressions.

% \subsection{Contextual LF}\label{sec:clf}
% Contextual LF extends the logical
% framework LF \citep{Harper93jacm} with the power of contextual objects
% $\Psihat. M$ of type $A[\Psi]$. $M$ denotes an object which may refer to the
% bound variables listed in $\Psihat$ and has type $A$ in the context
% $\Psi$ (see also \citep{Nanevski:ICML05}).$\Psihat$ can be obtained from the
% context $\Psi$ by simply dropping the type annotations and keeping only the
% declared variable names. We characterize only objects in $\beta\eta$ normal
% form, since these are the only meaningful objects in LF. Furthermore, we
% concentrate here on characterizing well-typed terms, but defining kinds and
% kinding rules for types is straightforward and omitted.
%     \[ \begin{array}{lrcl}
% %     \mbox{Sorts} & s & \bnfas & \ttype \mid \tkind
%       \mbox{Atomic types} & P,Q & \bnfas & \app {\const a} {\vec M} \\ % \neutral{\const{a}}{S} \mid \Size \\
% %      \mbox{Types/kinds} & A,B,K & \bnfas & \ttype \mid P \bnfalt \Pi x{:}A.B %
% %      \mid \Pi x{:}\Size.B
%       \mbox{Types} & A,B & \bnfas &  P \bnfalt \Pi x{:}A.B % \mid \Pi x{:}\Size.B
% \\
%       \mbox{Heads}  & H & \bnfas & x \bnfalt \const{c} \bnfalt
%         p[\sigma] % \mid \const a 
% \\
%       \mbox{Neutral Terms} & R & \bnfas &  H \mid \app R N \bnfalt u[\sigma]  
% \\                                   
%       \mbox{Normal Terms} & M,N & \bnfas &  R  \mid \lam{x}{M}  
% \\
%      \mbox{Substitutions} & \sigma & \bnfas & \edot \bnfalt \id_\psi \bnfalt
%      \sigma, M \bnfalt \sigma ; H \\ % \bnfalt s[\sigma]\\
%       \mbox{Contexts} & \Psi & \bnfas & \edot \bnfalt \psi \bnfalt
%       \Psi, x{:}A
% % \\
% % \mbox{Signature} & \Sigma & \bnfas & \cdot 
% %   \bnfalt \Sigma, \const a{:}K 
% %   \bnfalt \Sigma, \const c{:}A 
% % \\[1ex]
% \end{array}\]


% \begin{center}
%   \begin{tabular}{ll}
% $\Delta ; \Psi \der M \chk A$ & Normal term $M$ checks against type $A$ \\
% $\Delta ; \Psi \der R \syn A$ & Neutral term $R$ synthesizes type $A$ \\
% $\Delta ; \Psi \der \sigma \chk \Psi'$ & Substitution $\sigma$ has domain $\Psi'$ and range $\Psi$. \\
% % $\Delta ; \Psi    \der A \chk s$ & LF types and kinds are well-formed \\
%   \end{tabular}
% \end{center}

% \begin{figure}
% \[
% \begin{array}{l}
%      \mbox{Neutral Terms}\quad \fbox{$\Delta;\Psi \der R \syn A$}
%       \hfill
% \\[0.7em]
%      % \quad
%       % \infer{\Delta; \Psi \entails \const a \syn K}
%       %       {\Sigma(\const a) = K}
%       \quad
%       \infer{\Delta; \Psi \entails x \syn A}
%             {\Psi(x) = A}
% \quad
%       \infer{\Delta; \Psi \entails p[\sigma] \syn [\sigma]_\Phi A}
%             {\Delta(p) = \# A[\Phi] & 
%              \Delta; \Psi \entails \sigma \chk \Phi }
%      \\[1em] 
%       \infer{\Delta; \Psi \entails \const c \syn A}
%             {\Sigma(\const c) = A}
% \quad
%      \infer{\Delta; \Psi \entails u[\sigma] \syn [\sigma]_{\Phi} P}
%             {\Delta(u) = P[\Phi]  % & \Delta; \Psi \entails S :
%               & \Delta ; \Psi \entails \sigma \chk \Phi }
%       \\[1em]
%       \infer{\Delta; \Psi \entails R\,M \syn [M/x]_A B 
%            }{\Delta; \Psi \entails R \syn \Pi x{:}A.B &
%              \Delta; \Psi \entails M \chk A}
%       \quad
% \\[1em]
%       \mbox{Normal Terms}\quad\fbox{$\Delta;\Psi \der M \chk A$}\hfill
% \\[0.7em]
%       \infer{\Delta; \Psi \entails R \chk Q
%            }{\Delta; \Psi \entails R \syn P & P = Q }
%       \quad
%       \infer{\Delta; \Psi \entails \lam{x}{M} \chk \Pi x{:}A.B}
%             {\Delta; \Psi, x{:}A \entails M \chk B}
%       \\[1em]
%       \mbox{Substitutions}\quad\fbox{$\Delta;\Psi \der \sigma \chk
%         \Psi'$} \hfill\\[0.5em]
%       \infer{\Delta; \Psi \entails \edot \chk \edot}{}
%       \quad
%       \infer{\Delta; \Psi \entails \sigma ; H \chk \Phi,x{:}A}
%             {\Delta; \Psi \entails \sigma \chk \Phi & \Delta ; \Psi \entails H \syn B & B = [\sigma]_{\Phi}A}
%      \\[1em]
%       \infer{\Delta; \psi,\Psi \entails \id_{\psi} \chk \psi}{}
%       \quad
%       \infer{\Delta; \Psi \entails \sigma , M \chk \Phi,x{:}A}
%             {\Delta; \Psi \entails \sigma \chk \Phi & 
%               \Delta; \Psi \entails M \chk [\sigma]_{\Phi} A}
% \end{array}
% \]
% \caption{Typing for contextual LF}
% \label{fig:lftyping}
% \end{figure}

%     \[ \begin{array}{lrcl}
%  \mbox{Context schemas} & G & \bnfas & \exists \wvec{(x{:}A)} . B  \mid G + \exists \wvec{(x{:}A)} . B \\
%       \mbox{Meta Objects} & C & \bnfas & \Psihat. R \bnfalt \Psi % \bnfalt \Psihat.\sigma
% \\
%       \mbox{Meta Types} & U & \bnfas & P[\Psi] \bnfalt \#A[\Psi] % \bnfalt \Psi[\Phi]
%         \bnfalt G
% \\ 
% %      \mbox{Free variable contexts} & \Phi & \bnfas & \edot \bnfalt
% %      \Phi, X:A\\
%       \mbox{Meta substitutions} & \theta & \bnfas & \edot \bnfalt
%         \theta, C / X
% %        \bnfalt \theta, X  % BP: we need the identity to push theta through an mlam. where we do not
%                            % have the type of X available to appropriately expand meta-variables.
% \\    
%   % \theta, \Psihat.R \bnfalt \theta, \Psihat.x  \bnfalt \theta ; q\\[0.5em]
% \mbox{Meta-context} & \Delta & \bnfas & \edot \bnfalt 
%    \Delta, X{:}U
% %  \Delta, u{:}P[\Psi] \bnfalt \Delta, p{:}A[\Psi]  \bnfalt \Delta , \psi {:}W\\
%     \end{array} \]


% \begin{figure}
%     \begin{rules}
% \mbox{Meta Terms}\quad \fbox{$\Delta \der C \chk U$}
% \hfill 
% \\
% \hfill\infer{\Delta \der {\cdot} \chk G}{}
% \quad
% \infer{\Delta \der \psi \chk G
%      }{\Delta(\psi) = G}
% \\[0.5em]
% \infer{\Delta \der \Psi, x{:}A \chk G
%      }{
%        \begin{array}{l}
%        \Delta \der \Psi \chk G 
% \\        
%        \exists \wvec{(x:B')}.B \in G \quad
%        \Delta; \Psi \der \sigma \chk \wvec{(x{:}B')} \quad
%        A = [\sigma]_{\wvec{(x:B')}} B 
%        \end{array}
%      } 
% \\[1em]       
% %\mbox{Meta-Terms}\hfill \\
% \infer{\Delta \der \Psihat. \sigma \chk \Phi[\Psi]
%      }{\Delta; \Psi \der \sigma \chk \Phi}
% \quad
% \infer{\Delta \der \Psihat. R \chk P[\Psi]
%      }{\Delta; \Psi \der R \chk P}
% \quad
% \infer{\Delta \der \Psihat. x \chk \#A[\Psi]
%      }{% \Delta;\Psi \der \Psi(x) = A : \ttype
%          \Psi(x) = A}
% \\[1em]
% \infer{\Delta \der \Psihat. p[\pi] \chk \# B[\Psi] }{
%        \Delta(p) = \# A[\Phi] & 
% %       \Delta ; \Psi \der p[\sigma] \syn A &
% & 
% %       \Delta; \Psi \der A = B : \ttype
%          \Delta ; \Psi \der \pi \chk \Phi &
%          [\pi]_{\Phi}(A) = B
%        }
% %\\[0.5em]
% %\mbox{where}\;\pi\;\mbox{is a variable substitution} 
% \\[1em]
% \mbox{Meta-Substitutions} \quad \fbox{$\Delta \der \theta \chk \Delta'$}\hfill \\[0.7em]
% %\framebox[10cm]{
% \infer{\Delta \vdash \cdot \chk \cdot}{} 
% \quad
% \infer{\Delta \der \theta, C/X \chk \Delta', X{:}U
%      }{\Delta \der \theta \chk \Delta' &
%        \Delta \der C \chk \msub{\theta}_{\Delta'}(U)
%      }
%    \end{rules}
% \caption{Typing for meta-terms}
% \label{fig:metatyping}
% \end{figure}

I give below a brief overview of the results of this work, which
describes the integration of LF-indexed recursive datatypes into 

Indexed recursive types have been considered before
by e.g. \cite{Zenger:TCS97} and \cite{Xi99popl}. There were two main
novel aspects of our work:
%1) A modular description of the index
%language and the computation language allows one to consider
%alternative index languages with minimal impact on the metatheory or
%implementation of the computation language
1) the application of these techniques to an index language
supporting notions binding and substitution; i.e. an index language
(in this case, Contextual LF) rich enough to talk about object
logics. 2) The treatment of index contraints, and our treatment of
pattern matching and refinement of indices is simpler than other
known approaches. The resulting language is a prime candidate for
implementing code transformations, algorithms over abstract syntax,
and (partial) verification.

I briefly recap the main technical aspects of this work. The
language of (computation-level) types is presented below, with the
most important type formers highlighted. I write $U$ to stand for an index
language type, $\mu$ contructs indexed recursive types, and $C_1 = C_2
\wedge T$ expresses an equational constraint on indices together with
a type.

\[
\begin{array}{l@{~}r@{~}c@{~}l}
% \mbox{Recursive Type} & S & \bnfas & 
%          C = C \bnfalt T 
%   \bnfalt S_1 & S_2 
%   \bnfalt S_1 + S_2  \bnfalt \exists X:U. S 
%
%  \\[2pt] 
\mbox{Kinds} & K & \bnfas & \ctype \bnfalt \pibox X U K \\

\mbox{Types} & T  & \bnfas & 
        \m{Unit} 
\bnfalt Z 
\bnfalt T_1 \arrow T_2   
\bnfalt  T_1 \times T_2 
\bnfalt \dsum{\wvec{l : T}} 
\\
& & &   
\bnfalt \pibox X U T 
\bnfalt \sigmabox X U T 
\bnfalt {\color{blue} C_1 = C_2 \andalso T}
\\
&&&    
 \bnfalt {\color{blue} \mu Z.\,\mlam {\vec X}  T}
 \bnfalt {\color{blue} T\;{\vec C} }
% \bnfalt T\;C
\bnfalt U
% \mbox{Definition} & D & \bnfas & \lambda X.D \bnfalt \bnfalt \bnfalt T
\\[2pt] %  
\mbox{Context} & \Gamma & \bnfas & \cdot \bnfalt \Gamma, Z:K \bnfalt \Gamma, x:T
\end{array}
\]

Below, I illustrate how to write a type of lists of booleans indexed by their
length, which we call vectors. The type of vectors is defined
recursively: either the index $X$ is 0 and we are at the end of the
list or there is some $Y$ such that $X$ is the successor of $Y$, then
a boolean and, recursively, a vector of indexed by $Y$.

\[
\begin{array}{r@{\;}c@{\;}l@{~:~}l}
% \nat:& \multicolumn{2}{l}{\ttype.}\\ \relax
% 0 : & \multicolumn{2}{l}{\nat.}\\\relax
% \m{s}: & \multicolumn{2}{l}{\nat \arrow \nat.}\\[1em]
%
\text{Vec} = \mu \text{Vec}. \lambda X. & \langle & \m{nil} &  X = 0 \andalso \tunit\;, \\
&  & \m{cons} &  \sigmabox Y \nat X = \m{s}\;Y \andalso \m{bool} \times \text{Vec}\;Y  \;\rangle 
\end{array}
\]

For our purposes, we are interested in using such indexed recursive
types enforcing invariants about syntax and related notions such as contexts.
For example, one often has the need to express the notion of a relation
between contexts when relating two languages. A typical example might relate a context
which lists both term variables and type variables to a corresponding
context which lists only the type variables (e.g. when representing
System F). Below we write this relation in surface syntax of Beluga and the
corresponding form in the core theory. 

% \begin{lstlisting}
% datatype ctx_rel : ctx -> cctx -> ctype
% | rnil : ctx_rel [] []
% | rsnoc : ctx_rel \psi$\;$\phi -> ctx_rel (\psi,x:tm) (\phi,x:ctm)
% \end{lstlisting}

\begin{lstlisting}
datatype ctx_rel : tmctx -> tpctx -> ctype
| nil : ctx_rel [] []
| tmsnoc : ctx_rel \psi$\;$\phi -> ctx_rel (\psi,x:tm) \phi
| tpsnoc : ctx_rel \psi$\;$\phi -> ctx_rel (\psi,a:tp) (\phi,a:tp)
\end{lstlisting}

% \[
% \begin{array}{r@{\;}c@{\;}l@{}l}
% \mu \text{Ctx\_rel}. \lambda \psi\lambda \phi. & \langle & \m{nil}\; &:  \psi
% = \cdot \andalso \phi = \cdot \andalso \tunit\;, \\
% &  & \m{snoc} & : \Sigma \psi'{:}{\m{ctx}}. \Sigma \phi'{:}{\m{cctx}}.\\ 
% &&& \psi = \psi', x{:}\m{tm} ~\andalso~ \phi = \phi', y{:}\m{ctm} \\
% &&& \andalso\;\text{Ctx\_rel}\;\psi'\;\phi' \;\rangle 
% \end{array}
% \]

Here is its corresponding representation in the core language: 

\[
\begin{array}{r@{\;}c@{\;}l@{}l}
\mu \text{Ctx\_rel}. \lambda \psi\lambda \phi. & \langle & \m{nil}\; &:  \psi
= \cdot \andalso \phi = \cdot \andalso \tunit\;, \\
&  & \m{tmsnoc} & : \Sigma \psi'{:}{\m{tmctx}}. \psi = \psi', x{:}\m{tm}
\andalso\;\text{Ctx\_rel}\;\psi'\;\phi ,\\
&  & \m{tpsnoc} & : \Sigma \psi'{:}{\m{tmctx}}. \Sigma
\phi'{:}{\m{tpctx}}. \psi = \psi', x{:}\m{tp} ~\andalso~ \phi = \phi',
y{:}\m{tp} \\
&&& \andalso\;\text{Ctx\_rel}\;\psi'\;\phi' \;\rangle 
\end{array}
\]

This is a typical example of
functional relation. We will often be obligated to prove manually
that it is total and deterministic, which would be unnecessary if it
were represented directly as a function, such as the one
below. However, while the language in this paper supports writing such
functions, it does not support \emph{reasoning} about them, since they
cannot occur in indices. So such a function is of limited
utility; it cannot completely replace the corresponding relation.
\begin{eqnarray*}
|-| & : & \text{tmctx} \rightarrow \text{tpctx} \\
| \psi , x:\text{tm} | & = & | \psi | \\
| \psi , a:\text{tp} | & = & | \psi | , a:\text{tp}
\end{eqnarray*}

I present below the syntax for the computation language. The
corresponding typing rules can be found in Fig. \ref{fig:comptyping}. This
is mostly standard. The recursive types are \emph{iso-recursive}
types, so \lstinline{fold} contructs an inhabitant of such types, and
they are destructed by pattern matching. Case branches $B$ have a
\emph{refinement substitution} $\theta$ which describes how variables
of the index language are refined with more information when a
particular pattern matches. \LONGVERSION{TODO: Explain in more detail}

\[
\begin{array}{l@{~}r@{~}c@{~}l}
\mbox{Expressions} & I  & \bnfas & y 
  \bnfalt \app I E % \bnfalt \fst I \bnfalt \snd I 
  \bnfalt \app I \ctxi C 
%  \bnfalt \unfold I
   \bnfalt  (E : T)\\
\mbox{~(synth.)} & & & 
% \Bnfalt  \letpack{u}{x}{e}{e'}
\\[2pt]
\mbox{Expressions} & E & \bnfas &  I 
%  \bnfalt \boxm{\Psihat}{R} 
  \bnfalt C % \tbox\, C
  \bnfalt \fn{y}{E}
  \bnfalt \mlam{X}{E} 
  \bnfalt \rec{f}{E} 
  \bnfalt \m{unit}\\
\mbox{~(checked)}& & &   \bnfalt \fold E  \bnfalt \dsum{l=E}   \bnfalt \pack (C , E) \\
% & & &   \bnfalt \letpack {(X , y) = I} E
&&&  \bnfalt (E_1, E_2) \bnfalt \casebox{I}{\vec{B}} \\
\mbox{Pattern } & {pat} & \bnfas & x \bnfalt C \bnfalt \m{unit} \bnfalt \fold {pat} \bnfalt \dsum{l = {pat}} \\
                         &          &            & \bnfalt \pack (C , {pat}) \bnfalt ({pat}_1, {pat}_2)\\
\mbox{Branch  } & B  & \bnfas &  \casearm{\Delta ; \Gamma\;.\; pat : \theta}{E}
%%&&& 
%%                         \bnfalt \casearm{\Delta ; \Gamma\;.\;pat : \theta}{\impos} 
\\[2pt]
\mbox{Contexts} & \Gamma  & \bnfas &  \edot \bnfalt \Gamma, y{:}T \\
\end{array}
\]  

\begin{figure}\label{fig:comptyping}
  \centering
\[
\begin{array}{l}
\multicolumn{1}{l}{\fbox{$\Delta;\Gamma \der I \syn T$}\quad
\mbox{Expression $I$ synthesizes type $T$}}\hfill\\[0.8em]
\infer{\Delta; \Gamma \entails y \syn T}
      {y{:}T \in \Gamma} 
\quad
\infer{\Delta ; \Gamma \der I \syn T}{
       \Delta ; \Gamma \der I \syn C = C \andalso T
}
 \\[0.8em]
 ~~~
\infer
 {\Delta ; \Gamma \entails I\;E \syn T}{
  \Delta ; \Gamma \entails I \syn T_2 \arrow T ~~&~~
  \Delta ; \Gamma \entails E \chk T_2 }
 \\[0.8em]
 \quad
\infer
  {\Delta ; \Gamma \entails \app I \ctxi C \syn \msub{C/X}T}
  {\Delta ; \Gamma \entails I \syn \pibox X U T
    ~~&~~ 
   \Delta \entails C \chk U }
%\\[0.7em]
\quad
\infer
   {\Delta ; \Gamma \entails (E : T) \syn T}
   {\Delta ; \Gamma  \entails E \chk T }
% \quad
% \infer{\Delta ; \Gamma \der \unfold I \syn [T/Z]\msub{\vec C / \vec X}S}{
%        \Delta ; \Gamma \der I \syn T\;{\vec C} & 
%        T = \mu Z.\,\mlam {\vec X} S
% }
\\[0.7em]
\multicolumn{1}{l}{\fbox{$\Delta;\Gamma \der E \chk T$}\quad
\mbox{Expression $E$ checks against type $T$}}\\[0.8em]
\infer{\Delta ; \Gamma \der \dsum{l_i=E_i} \chk \dsum{\wvec{l:T}}}
      {\Delta ; \Gamma \der E_i \chk T_i\quad\mbox{where}~l_i:T_i \in \wvec{l:T}}
% \\[0.8em]
%\\[1em]
\quad
\infer %[\Delta = \vec X : \vec U(\vec X)]
   {\Delta; \Gamma ~\entails~  \rec{f}{E} ~\chk~ T }
   {\Delta ;  \Gamma, \, f : T
     ~\entails~ E ~\chk~ T}
\\[0.8em]
\infer
    {\Delta; \Gamma \entails  \fn{y}{E} \chk T_1 \arrow T_2}
    {\Delta; \Gamma, y{:}T_1 \entails E \chk T_2 } 
\quad
% \\[0.8em]
\infer
 {\Delta ; \Gamma \entails \mlam{X}{E} \chk \Pibox X{:}U.T }
 {\Delta, X{:}U ; \Gamma \entails E \chk T }
\\[0.8em]
%\quad
\infer{\Delta ; \Gamma \der (E_1,E_2) \chk T_1 \times T_2}{
       \Delta ; \Gamma \der E_1 \chk T_1 & 
       \Delta ; \Gamma \der E_2 \chk T_2
}
\quad
\infer
  {\Delta ; \Gamma \entails I \chk T' }
  {\Delta ; \Gamma \entails I \syn T & T = T'}
%\\[0.8em]
\\[0.8em]
% \infer{\Delta ; \Gamma \der \inl E \chk T_l + T_r}{\Delta ; \Gamma \der E \chk T_l }
% \quad
%  \infer{\Delta ; \Gamma \der \inr E \chk T_l + T_r}{\Delta ; \Gamma \der E \chk T_r}
% \\[1em]
%\\[1em]
\infer{\Delta ; \Gamma \der \pack (C , E) \chk \sigmabox X U T}{
       \Delta \der C \chk U & 
       \Delta ; \Gamma \der E \chk \msub{C/X}T
}
\quad
 \infer{\Delta; \Gamma \entails % \tbox\,
          C \chk U}
       {\Delta \entails C \chk U }
\\[0.8em]
\infer{\Delta ; \Gamma \der \fold E \chk  (\mu Z. \mlam {\vec X}  S)\;{\vec C}}{
%       T = \mu Z. \mlam {\vec X} . S& 
       \Delta ; \Gamma \der E \chk [\mu Z. \mlam {\vec X} S/Z]\msub{\vec C/\vec X}S}
%\quad
\quad
\infer{\Delta ; \Gamma \der E \chk C = C \andalso T}{
       \Delta ; \Gamma \der E \chk T
}
\\[0.8em]
% \infer{\Delta ; \Gamma \der \letpack {(X,y) = I} E \chk T}{
%        \Delta ; \Gamma \der I \syn \sigmabox X U S & 
%        \Delta, X{:}U ; \Gamma,y {:}S \der E \chk T
% }
% \\[1em]
\infer
   {\Delta; \Gamma \entails \casebox{I}{\vec B} \chk T }
   {\Delta; \Gamma \entails I \syn S % T\;\vec C
     &
     \mbox{for all}~i~ 
    \Delta; \Gamma \entails B_i  \chkbranch{S} T  %& 
%    \Delta; \Gamma \entails B_r  \chkbranch{S_r} T  %& 
%      \Delta; \Gamma \entails B_r  \chkbranch{S_r} T  
} 
\\[0.8em]
\multicolumn{1}{l}{\fbox{$\Delta ; \Gamma \der B \chkbranch{S} T$}\quad
\mbox{Branch $B$ with pattern of $S$ checks against $T$}} \\[0.8em]
\infer
  { \Delta ; \Gamma \entails \Delta_i ; \Gamma_i\,.\,{pat} : \theta_i \mapsto E \chkbranch{S} T}
  {% () \not\in {pat} &
%   \begin{array}{c}
  \Delta_i  \entails  \theta_i  \chk \Delta  & 
    \Delta_i ; \Gamma_i \der {pat} \chk \msub{\theta_i} S &
    \Delta_i ; \msub{\theta_i}\Gamma, \Gamma_i \entails E \chk  \msub{\theta_i}T  
   }
%\\[1em]
%\infer
%  { \Delta ; \Gamma \entails \Delta_i ; \Gamma_i\,.\,{pat} : \theta_i \mapsto \impos \chkbranch{S} T}
%  {() \in {pat} & \Delta_i ; \Gamma_i \der {pat} \chk \absurd \msub{\theta}S & 
%   \Delta_i  \entails  \theta_i  \chk \Delta  }
%\\[1em]
%\end{array}
%\]
%\[
%\begin{array}{l}
\end{array}
\]   
%
  \caption{Typing for computations}
  \label{fig:typcomp}
\end{figure}

The main technical result in this paper is a machine-checked proof of type
soundness for this language, which I developed in the Coq proof assistant.

This work was an important milestone in understanding how HOAS can fit
into the more general setting of an expressive type theory, and how it
can be used effectively in implementing algorithms over syntax with
sophisticated invariants enforced by the type system. However, it left open a few
questions, first how to transition from indexed recursive types to
\emph{inductive types} over LF representations, and closely related, how to support termination analysis for
such types. These steps are crucial if we wish to transition from
programming with type-enforced invariants to \emph{proving}. Another question is whether we can move
from indexed types to \emph{dependent types}, which support reasoning
about the functions we write.

\subsection{First-class substitutions}\label{sec:lfmtp13}
The LFMTP'13 paper \citep{Cave13} addresses direct support for
simultaneous (sometimes known as parallel)
substitutions, which is a notion that arises in many programming
language proofs. However, previously this notion had been
conspicuously absent from work on logical frameworks and HOAS. This is
likely because this notion arises most commonly in logical relations
proofs, which are often out of reach of HOAS-based systems. To
illustrate the benefit from direct support for
simultaneous substitutions, this paper describes the formalization of such
a logical relations proof, using one of the simplest
imaginable such proofs: weak (head) normalization for the
simply-typed lambda calculus. This formalization appears to be one of
the most direct and succinct formalizations of a logical relations
proof to date, which demonstrates that the framework described in the
paper shows significant promise for formalizing more sophisticated results. I briefly recap
this formalization here. 

% \begin{quote}
% If $\Gamma \vdash M : A$ and $\sigma \in \mathcal{R}_\Gamma$ then
% $[\sigma]M \in \mathcal{R}_A$.
% \end{quote}

% Here we have quantified over all substitutions $\sigma$ providing
% instantiations for all the free variables in $M$. This is a capability
% which is often not present in systems with rich support for
% substitutions, since the context $\Gamma$ is often ambient instead of
% available explicitly. 

% In such proofs, there is typically a need to
% prove equations about simultaneous substitutions, for example:
% $[N/x][\sigma,x/x]M = [\sigma,N/x]M$ provided $x \not\in
% fv(\sigma)$. Our work in \citep{Cave13} solves this issue by embedding
% a decision procedure for these sorts of equations into the proof checker.

\subsubsection{Hand written proof}

Here I go over the informal presentation of the proof to explain
the translation to a HOAS-based formalization, and to draw attention
to the aspects one often ignores in a paper proof, but which have
significant impact on a formalization.

The grammar for the calculus includes lambda abstractions, variables, application, and a single
constant $\const c$ of a constant type $\const i$. Lambda abstractions and the
constant $\const c$ are considered values.

\newcommand{\stepsto}{\longrightarrow}

\[
\begin{array}{lrcl}
\mbox{Types} & A,B & \bnfas & {\const i} \bnfalt A \arrow B \\
\mbox{Terms} & M,N & \bnfas & {\const c} \bnfalt x \bnfalt \lam x M \bnfalt M\;N \\
\mbox{Values} & V & \bnfas & {\const c} \bnfalt \lam x M
\end{array}
\]

The typing rules are standard and use the judgment $\Gamma \vdash M : A$ where
$\Gamma$ describes a typing context.

\[
\begin{array}{l}
\infer{\Gamma \vdash x : A}{\Gamma(x) = A}
\quad
\infer{\Gamma \entails \lam x M : A \arrow B}
      {\Gamma,x{:}A \der M : B}
\quad
\infer{\Gamma \entails {\const c} : {\const i}}{}
\quad
\infer{\Gamma \entails M\; N : B}
      {\Gamma \entails M : A \arrow B & \Gamma \entails N : A}
\end{array}
\]

We then define weak head reduction below. We say that a term $M$ halts if there exists a value $V$ such that $M
\stepsto^* V$. 

\[
\begin{array}{l}
\infer[\mathsf{beta}]{(\lam x M) N \stepsto [N/x]M}{}
\quad
\infer[\mathsf{app}]{M\;N \stepsto M'\;N}{M \stepsto M'}
\end{array}
\]


We can now define the notion of reducibility which is typical such logical
relations proofs, which goes back to \cite{Tait67}. A term is reducible at base type precisely
when it halts. A term $M$ is reducible at arrow type $A \arrow B$ when it halts, and
for every reducible $N$ at type $A$, the application of $M$ to $N$ is
reducible at type $B$.

\[
\begin{array}{l}
\mathcal{R}_{\const i} = \{M\; |\; M \text{ halts}\} \\
\mathcal{R}_{A \arrow B} = \{M\; |\; M \text{ halts and } \forall N
\in \mathcal{R}_A, (M\; N) \in \mathcal{R}_B \}
\end{array}
\]

It is trivial from the definition that if a term $M$ is reducible at
some type $A$, then it halts. One can easily prove that
$\mathcal{R}_A$ is closed under
expansion by induction on the type:

\begin{lemma}[Closure under expansion]
If $M' \in \mathcal{R}_A$ and $M \stepsto M'$ then $M \in \mathcal{R}_A$
\end{lemma}

Our aim is to show that all well-typed closed terms are reducible, but as
usual we must generalize to showing that all closed instantiations of
a well-typed open term are reducible. For this we need the notion of a
simultaneous substitution, and to define reducibility of
substitutions. If $\sigma = M_1/x_1,M_2/x_2,...,M_n/x_n$, we say
$\sigma$ is reducible at context $\Gamma = x_1{:}A_1,...,x_n{:}A_n$ if
each $M_i$ is reducible at $A_i$, i.e. $M_i \in \mathcal{R}_{A_i}$. We
write this as $\sigma \in \mathcal{R}_\Gamma$. We can now state the main
lemma:

\begin{lemma}[Main lemma]\label{lem:main}
If $\Gamma \vdash M : A$ and $\sigma \in \mathcal{R}_\Gamma$ then
$[\sigma]M \in \mathcal{R}_A$.
\end{lemma}
\begin{proof}
By induction on the typing derivation. 
We show only the interesting case:

Case $\begin{array}{l}\infer{\Gamma \der \lam x M : A \arrow
    B}{\Gamma,x{:}A \der M : B}\end{array}$:

First, $[\sigma](\lam x M) = \lam x ([\sigma,x/x]M)$ halts, since it
is a value. Suppose then that we are given $N \in \mathcal{R}_A$.

\begin{enumerate}
\item $[\sigma,N/x]M \in \mathcal{R}_B$ (by I.H.)
\item $[N/x][\sigma,x/x]M \in \mathcal{R}_B$ (property of
  substitution; $x$ assumed fresh for $\sigma$)
\item $(\lam x ([\sigma,x/x]M))\; N \in \mathcal{R}_B$ (by closure
  under expansion)
\end{enumerate}
Hence $[\sigma](\lam x M) \in \mathcal{R}_{A \arrow B}$ (by definition)
\end{proof}

\begin{corollary}[Weak normalization]
If $\der M : A$ then $M$ halts.
\end{corollary}
\begin{proof}
By the main lemma, we have that $[\cdot]M\in \mathcal{R}_A$
and hence that $[\cdot]M$, which is equal to $M$, halts.
\end{proof}

The first point to draw attention to is the quantification
over the context $\Gamma$ and the quantification over a simultaneous
substitution $\sigma$ providing instantiations for $\Gamma$. This
quantification typically cannot be directly expressed in HOAS-based
systems, since the context is typically ambient, so that direct
access to the context is impossible. This is the case in Twelf and
Delphin.

The second point to draw attention to in this proof is the use of
properties of simultaneous substitutions in the main
lemma and its corollary. On paper, these properties look intuitive and
innocuous, but as explained earlier, these properties are typically a source of
significant overhead when formalizing such results using various 
representations of variables and variable binding. However, these
properties are immediate in our framework, as
the equational theory of (simultaneous) substitutions is built into
our type theory. This means that the formalization need only implement
the main content of the proof as it is written on paper, and not these
technical details.

\subsubsection{Formalization}\label{sec:belugaweaknorm}

Here we demonstrate the particularly elegant encoding of this proof in
the Beluga system, extended with support for first
class (simultaneous) substitutions. By hand we defined grammar and
typing separately, but here it is more convenient to define
intrinsically typed terms directly. Below, \lstinline{tm} defines our family of simply-typed lambda
terms indexed by their type as an LF signature: term formers are
represented as LF constants. In typical higher-order abstract syntax fashion, lambda
abstraction takes a \emph{function} representing the abstraction of a
term over a variable. There is no case for variables, as they are
treated implicitly. We remind the reader that this is a weak, representational function space
-- there is no case analysis or recursion, and hence only genuine
lambda terms can be represented. To convince oneself of this fact, one can prove that this definition is
\emph{adequate}: that there is a (compositional) bijection between normal LF terms of
this type, and our informal notion of terms. We do not go into details
about this here.

\begin{lstlisting}
tp  : type.               
i   :  tp.
arr : tp -> tp -> tp.

tm  : tp -> type.
app : tm (arr A B) -> tm A -> tm B.
lam : (tm A -> tm B) -> tm (arr A B).
c   : tm i.
\end{lstlisting}

We can then encode our step relation in a similar fashion. Notice in
particular we use LF's application to encode substitution in the
\lstinline{s/beta} case. We define also which terms are values, and
what it means for a term to halt (not shown). 

\begin{lstlisting}
step  : tm A -> tm A -> type.
s/beta : step (app (lam (\x. M x)) N) (M N).
s/app  : step M M' -> step (app M N) (app M' N).
\end{lstlisting}

Reducibility cannot be encoded as an LF signature, since it
involves a genuine function space. Hence we move to the
computation layer of Beluga, and employ an
indexed recursive type, as defined in the previous section. Contextual LF objects
and contexts which are embedded into computation-level types and programs are 
written inside \lstinline![  ]!. 

Once again, a term of type \lstinline{i} is reducible if it halts, and a term
\lstinline{M} of
type \lstinline{arr A B} is reducible if it halts, and moreover for every
reducible \lstinline{N} of type \lstinline{A}, the application
\lstinline{app M N} is reducible. We write \lstinline!{N:[.tm A]}!
for explicit $\Pi$-quantification over \lstinline{N}, a closed term of type
\lstinline!A!. To the left of the dot in \lstinline{[.tm A]} is where
one writes the context the term is defined in -- in this case, it is empty.

\begin{lstlisting}
datatype Reduce : {A:[.tp]} {M:[.tm A]} ctype =
| I   : [. halts M] -> Reduce [. i] [. M]
| Arr : [. halts M] ->
        ({N:[.tm A]} Reduce [. A] [. N] -> Reduce [. B] [. app M N])
        -> Reduce [. arr A B] [. M];
\end{lstlisting}

In this definition, the arrows represent the usual computational
function space, not the weak function space of LF. We note that this
is \emph{not} an inductive definition: it is not (strictly) positive, since \lstinline{Reduce}
appears to the left of an arrow in the \lstinline{Arr} case. Here we note that this
definition is, in some sense, stratified by the \lstinline{tp} index: the recursive occurences of
\lstinline{Reduce} are at types \lstinline{A} and \lstinline{B} which
are smaller than \lstinline{arr A B}. This is how one justifies its
existence: the type is, in a sense, \emph{computed} by primitive
recursion on the index argument \lstinline{tp}. Beluga does not currently perform either a positivity check
or a stratification check, so in the present we must convince ourselves that this
predicate is well-defined. In Coq \citep{bertot/casteran:2004} and Agda
\citep{Norell:phd07} one typically justifies such definitions using a
universe (large elimination), which Beluga does not currently
support. I comment more on this point later.

Next we prove closure of \lstinline{Reduce} under expansion. Proofs
by induction take the form of recursive functions. We show the
statement of the lemma, but we do not go into detail about the proof,
since it follows the handwritten proof closely.

\begin{lstlisting}
rec closed : [. step M M'] -> Reduce [.A] [.M'] -> Reduce [.A] [.M] = ...
\end{lstlisting}

Now we arrive at the part of the proof requiring simultaneous
substitutions. The type of simultaneous substitutions is a new built-in
type former of the index language: we write \lstinline{g[h]} for the
type of substitutions which provide, for each variable in the context
\lstinline{g}, a term in context \lstinline{g}.

We must state precisely what it means for a substitution to be
reducible. We do this by employing another indexed recursive type: a predicate
expressing that the substitution was built up as a list of reducible
terms. This predicate is defined on substitutions of type
\lstinline{g[]}, i.e. taking variables in \lstinline{g} to closed
terms of the same type. In the base case, the
empty substitution is reducible. In the \lstinline{Cons} case, we read
this as saying: if \lstinline{#S} is a reducible substitution
(implicitly at type \lstinline{g[]}) and \lstinline{M} is a reducible
term at type \lstinline{A}, then \lstinline{#S} with \lstinline{M}
appended is a reducible substitution (implicitly at type
\lstinline{(g,x:tm A)[]} -- the domain has been extended with a
variable of type \lstinline{A}).

\begin{lstlisting}
datatype RedSub : (g:ctx){#S:g[]} ctype =
| Nil : RedSub [. ^$\,$ ]
| Cons : RedSub [. #S] -> Reduce [.A] [.M] -> RedSub [. #S, M ];
\end{lstlisting}

% We implicitly quantify over the context \lstinline!g! by using round
% parenthesis writing \lstinline!(g:ctx)!; we explictly quantify over substitution
% variables using curly braces writing \lstinline!{#S:g[]}!. 

Finally, our fundamental theorem is standard and takes the form we would expect
from the handwritten proof: if \lstinline{M} is a well-typed
term, and we provide a reducible substitution \lstinline{#S} with closed
instantiations for each of the free variables of \lstinline{M}, then
\lstinline{M #S} (that is, the application of \lstinline{#S} to
\lstinline{M}) is reducible. We proceed by induction on the
term. Again we show only the interesting \lstinline{lam} case. Clearly
the lambda abstraction halts, as it is a value (this is the first,
boxed argument to \lstinline{Arr} below). To show 
that applying it to any reducible term is reducible, we appeal directly to
closure under expansion, and the induction hypothesis for
\lstinline{M1} with the substitution extended with
\lstinline{N}. Recall that \lstinline{rN} is a proof that
\lstinline{N} is reducible.

\begin{lstlisting}
rec fund : {g:ctx}{M:[g.tm A]} RedSub [. #S] -> Reduce [.A] [. M #S] =
mlam g => mlam M => fn rs => case [g. M ..] of
| [g. lam (\x. M1 .. x)] =>
  Arr [. h/val s/refl val/lam]
   (mlam N => fn rN => closed [. s/beta]
    (fund [g,x] [g,x. M1 .. x] (Cons rs rN)))
...
\end{lstlisting}

Weak normalization is now a trivial corollary, taking \lstinline{M} to be a closed term and
\lstinline{#S} to be the empty substitution:

\begin{lstlisting}
rec weakNorm : {M:[. tm A]} [. halts M] = mlam M => main (fund [] [. M] Nil);
\end{lstlisting}

Notably, we did not have to concern
ourselves with the property of substitutions that we wrote explicitly
in the paper proof in the \lstinline{lam} case. Using a low-level representation
of substitution (e.g. de Bruijn indices) one must prove $[\sigma,N/x]M = [N/x][\sigma,x/x]M$
(where $x \not\in \FV(\sigma)$) by hand, and this is actually a large
bulk of the proof. In this setting, it is handled automatically
by normalization during typechecking. Roughly, the
term \lstinline{fund [g,x] [g,x. M1 .. x] (Cons rs rN)} proves
the reducibility of \lstinline{M1[#S, N]}, while
\lstinline{closed [. s/beta]} is expecting a proof of the reducibility
of
\lstinline{M1[#S[$\uparrow$], 1][id, N]}. Our decision procedure for
equality of index language expressions normalizes both to
\lstinline{M[#S, N]}, so that this typechecks. In this work, the
decision procedure for equality is described using the technique of
\emph{hereditary substitution} \citep{Watkins02tr}. 


The main technical achievement of this paper is the decidability of
typechecking in the presence of such equations on substitutions. The
remarkably succinct machine-checked proof in Beluga serves to
illustrate the effectiveness and utility of this type system. 

However, one unresolved issue only becomes more pressing with this work: giving a
fully formal treatment of the definition of the logical 
relation. It cannot be an inductive type, so it must be explained by
other means. This is straightforward in richer type theories/proof
assistants in which we have access to a \emph{universe}, but such
theories and systems do not provide near the same support for
substitution. Bridging this gap is one component of my proposed
work. Unfortunately, the technique of hereditary substitution appears
to not scale to such theories, so a more robust approach is needed.

\subsection{Logical relations in contextual type theory}
I am in the process of preparing for submission a larger case study of a formalization
of a logical relations proof using the techniques developed in 
POPL'12 and LFMTP13 papers. This work is presented in the draft
\citep{Cave14}. It describes a formalization of a proof of
completeness of an algorithm for deciding equality of simply-typed
lambda terms up to $\beta\eta$ equivalence. Such a proof is the core
of the proof of decidability of typechecking for LF
\cite{Harper03tocl}, and the general technique is known to scale to
much richer type theories (e.g. \cite{Abel11}).

I want to draw attention to the aspects of this proof which make it
challenging to formalize and not worry too much about the details of
the proof. (ditto for the logical relations proof above)

In this
section we give a brief overview of the motivation and high level structure of
the completeness proof of algorithmic equality. For more detail, we refer the
reader to \cite{Crary:ATAPL} and \cite{Harper03tocl}. Extensions of 
this proof are important for the metatheory of dependently typed systems such as
LF and varieties of Martin-L\"of 
Type Theory, where they are used to establish decidability of typechecking. The proof concerns three judgements:

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{}}
$\Gamma \vdash M \equiv N : A$ & terms M and N are declaratively equivalent at
type $A$ \\
$\Gamma \vdash M \Leftrightarrow N : A$ & terms M and N are
algorithmically equivalent at type $A$ \\
$\Gamma \vdash M \leftrightarrow N : A$ & paths M and N are
algorithmically equivalent at type $A$ \\
  \end{tabular}
\end{center}

Declarative equivalence (also called definitional equality) includes convenient but non-syntax directed rules
such as transitivity and symmetry, among rules for congruence,
extensionality and $\beta$-contraction. We will see the full definition
in Sec. \ref{sec:mechanization}. In particular, it may include
apparently type-directed rules such as extensionality at unit type:

$$
\infer{\Gamma \vdash M \equiv N : \mathrm{Unit}}{\Gamma \vdash M : \mathrm{Unit} &
  \Gamma \vdash N : \mathrm{Unit}}
$$

We do not consider such a rule in our development. For our present
purpose it merely adds uninteresting cases to the proof. However, such a rule
motivates the use of algorithmic type-directed equivalence. The rule
above relies on type information and hence the usual normalize-and-compare strategy
for deciding equivalence does not work in the presence of this rule. Algorithmic equivalence on the
other hand is directly implementable, since it is directed by the syntax of
types and terms. We define algorithmic term equivalence 
mutually with path equivalence, which is the syntactic equivalence of terms headed by
variables, i.e. terms of the form $x\,M_1\,...\,M_n$. 

In this paper, we show completeness of algorithmic equivalence for declarative
equivalence. As usual, the problem is what 
to do at function types, since we need that applying equivalent
functions to equivalent arguments yields equivalent results. This is the reason
for defining an intermediate logical relation, which we write:

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{}}
$\Gamma \vdash M \approx N : A$ & terms M and N are logically equivalent at
type $A$ \\
  \end{tabular}
\end{center}

This relation is defined by induction on the structure of the
type. The key case is at function type, where Crary defines:

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{~~~}l@{}}
$\Gamma \vdash M_1 \approx M_2 : A \Rightarrow B$ & iff & for all $\Delta \geq \Gamma$
and $N_1$, $N_2$, \\
& & if $\Delta \vdash N_1 \approx N_2 : A$\\
& & then $\Delta \vdash M_1\; N_1 \approx M_2\; N_2 : B$
  \end{tabular}
\end{center}

The quantification over extensions $\Delta$ of $\Gamma$ is essential
for establishing monotonicity, below. This Kripke-style monotonicity
is one of the reasons that this proof is more challenging than normalization proofs, where this
quantification can be avoided using other technical tricks. For
our formalization, we take a slightly different approach which 
better exploits the features available to us. We
instead quantify over an arbitrary $\Delta$ together with a
\emph{renaming} substitution $\pi$ which brings terms from $\Gamma$ to
$\Delta$, i.e. $\Delta \vdash \pi : \Gamma$. This is a substitution of
variables for variables, i.e. it embodies precisely the structural
rules of exchange, contraction, and most importantly for this proof, weakening.

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{~~~}l@{}}
$\Gamma \vdash M_1 \approx M_2 : A \Rightarrow B$ & iff & for all
$\Delta$, $N_1$, $N_2$, and renamings $\pi$ \\
& & if $\Delta \vdash N_1 \approx N_2 : A$ and $\Delta \vdash \pi : \Gamma$ \\
& & then $\Delta \vdash M_1[\pi]\; N_1 \approx M_2[\pi]\; N_2 : B$
  \end{tabular}
\end{center}

The high level goal is to establish that declaratively equivalent
terms are logically equivalent, and that logically equivalent terms
are algorithmically equivalent. The proof requires establishing a
few key properties of logical equivalence. The first is
monotonicity, which is crucially used for weakening logical
equivalence. This is used when applying terms to fresh variables.

As one can imagine, the introduction of more substitutions means that
the equations which arise in the proofs become even more complex, and
the desire to discharge them automatically becomes even more
pressing. For example, the most complex equation which arises in this
proof is the following:

$$
M[\sigma[\uparrow],1][\pi[\uparrow],1][\text{id}, N] = M[\sigma[\pi],N]
$$

Fortunately, the LFMTP'13 work dutifully discharges this equation, and the user
does not even encounter the complex expression on the left.

\begin{lemma}[Monotonicity]
If $\Gamma \vdash M \approx N : A$ and $\Delta \vdash \pi : \Gamma$ is
a renaming substitution, then $\Delta \vdash M[\pi] \approx N[\pi] : A$
\end{lemma}

\begin{theorem}[Fundamental theorem]
If $\Gamma \vdash M \equiv N : A$ \\ and $\Delta \vdash \sigma_1 \approx
\sigma_2 : \Gamma$ then $\Delta \vdash M[\sigma_1] \approx
N[\sigma_2] : A$
\end{theorem}

By establishing the relatedness of the identity substitution to
itself, i.e. $\Gamma \vdash \mathrm{id} \approx \mathrm{id} : \Gamma$ we can combine the
fundamental theorem with the main lemma to obtain completeness.

\begin{corollary}[Completeness]
If $\Gamma \vdash M \equiv N : A$ then $\Gamma \vdash M
\Leftrightarrow N : A$
\end{corollary}

Our approach works well for the setting of indexed types we develop,
but it does not scale well to dependent types.

In this work we give the first example of a mechanized logical
relation proof using HOAS which has computational behaviour: its
computational content is a normalization algorithm.

\section{Remaining to be done}
\subsection{Computation in types}
Roughly speaking, my goal is to bring the worlds of HOAS-based proof
assistants together with the world of rich dependent type theory. My
work to date lays out the groundwork for such a plan, and gives us
reason to believe it is possible. This requires an explanation of how
higher-order abstract syntax is actually first-order.

The wisdom from LF is that if we understand and develop the theory for
a single, sufficiently powerful language (LF), we have along the way
developed the tools necessary for a wide variety of languages; we need
only encode them as ``sublanguages'' of LF: technically, by
considering a particular LF signature (list of basic constants). The
``higher-order'' aspect of these encodings is a red herring, and
in some sense obstructs our understanding of how to do ``standard''
mathematical analysis of such encodings.

My goal for the future is to integrate a logical framework into
dependent type theory. This provides an avenue to solve several of the
above mentioned issues:


1) Expressing functional relations on contexts as genuine
functions. Many other examples of ``computation in types''.

2) An avenue for analyzing termination

3) Justifying the reasonableness of logical relations (universes)

4) Reuse understanding of inductive types

This is necessary to support state-of-the-art proofs involving polymorphism,
universes, ``generic programming'', dependent types, step indexing.

basic design sketch

So do this *once* for LF, and embed your OL inside

I've prototyped this in Agda

$>$48GB of memory to typecheck an example

And we still have to apply these lemmas manually and often compose several of them

Solution: build LF datatype into DTT and build lemmas into conversion check

Result: ``dependently typed Beluga''

Analyze LF objects by the induction principle you get from considering an inductive type representing LF

provides ``simple'' approach to semantics, termination (via eliminator for LF datatype)?

The goal is (success is measured by) an algorithm for typechecking,
together with a proof of this algorithm's correctness: termination,
soundness, and completeness. The standard technique for proving this
kind of result about a dependent type theory also implies a few very
important corollaries: consistency of the theory, as well as
normalization of the calculus as a programming language.

This proof is highly non-trivial. This is ``Deep Theory''

technique for induction principle: induct over ``all of LF'' uniformly, permitting mutual definitions, etc.

technical results I am aiming to achieve: decidability
  of typechecking, (strong) normalization

Algorithm for typechecking: \citep{Coquand91,Harper05,Abel11}

Along lines of DML, ATS: specialized for metatheory. ``New equations
for neutral terms'' \cite{Allais13}


We give up a bit: intend to build around a de Bruijn
representation. Argue that for proofs it doesn't matter much:
typically we deal with one or two binders at at time. Scoping is
enforced by typechecking, which guides you where to insert
shifts. It actually takes quite a bit of effort to get shifts wrong. 

The second main component is to implement a prototype sufficient to
check some non-trivial proofs. I am to formalize 

\cite{Coquand98} normalization proof for Martin-L\"of type theory with a universe

soundness and completeness of a normalization by evaluation algorithm
as presented by \cite{Dybjer00}. 

These are substantial proofs which, to the best of my knowledge, have never been
formalized in a proof assistant with sufficient support for
substitution and binding. I aim to do these proofs because I have carried out these formalizations in
Agda, with gaps in the proof pertaining to substitutions. I have found
that these proofs are quite manageable once they are understood, with
the main exception of substitutions, which, for these proofs, will be
entirely handled by support for the equational theory of
substitutions, as described in Section \ref{sec:lfmtp13}.

If it turns out that such proofs will not work for some reason, there
are alternative candidates to illustrate the point: a logical relation
for contextual equivalence (parametricity) in the style of
\cite{Dreyer11} or normalization for (perhaps a predicative variant
of) System F \citep{ProofsAndTypes,Altenkirch93}.

intensional type theory is a language of verifiable evidence
(McBride's term?)

Alternative approaches: may ask why a language design? aren't there
already tools we can use to do this? e.g. proof by reflection, CoqMT,
VeriML?

Decidability of conversion with a universe: \cite{Abel07}

\section{Related Work}
...

\section{Timeline}
\paragraph{Stage 1:  May 2014 to Nov 2014} working out the metatheory and examples of computation in
types. Optimistically, a POPL paper in July. ESOP deadline in
October. 

\paragraph{Stage 2: Nov 2014 to May 2014} prototype implementation and implementing examples

 - Implementing examples is key to demonstrating the value of this
 approach. I have conducted many such proofs in Agda: There are three
 main practical obstacles to carrying out such proofs 1) one needs a
 sufficient understanding of techniques for dealing with de Bruijn
 indices 2) one of course needs to understanding the proofs 3)
 applying the equational theory of simultaneous substitutions. 1 and 2
 I have tackled, and 3 is still a burden: this is what I propose to
 address

CoqMT approach to implementation?
\paragraph{Stage 3: May 2015 to Nov 2015} writing thesis.

backup/option: Some kind of ``initial algebra semantics'' for hypothetical judgments?
optional piece: support for (and examples of) step-indexed logical relations
(Journal version of rec types paper combined with subst. vars?)

Also: Set aside time for writing up other stuff like journal version
of paper.

\bibliographystyle{plainnat}
\bibliography{proposal}
\end{document}