\documentclass{article}
\usepackage{natbib}
\usepackage{color}
\usepackage[pdftex, pdfborderstyle={/S/U/W 0}]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{alltt}

\newtheorem{theorem}{Theorem}

\author{Andrew Cave}
\title{Dependent type theory for contextual reasoning}

\definecolor{light-gray}{gray}{0.5}
\newcommand{\LONGVERSION}[1]{{\color{light-gray}#1}}

\begin{document}
\maketitle

\section{Introduction}

%(todo: look at other examples of thesis statements)

Increasingly, there is a desire to write computer checked proofs for
research in programming languages and logic
\citep{POPLMark}. Results in programming languages appear to be prime
candidates for computer checked proofs. For one, these proofs typically take the
form of a proof by induction with many cases, and formalizing inductive
reasoning is by now quite well understood. Moreover, the abundance of cases means that such proofs can be
tedious and error-prone to write and to verify by hand. Fortunately, it is now commonplace for proof
assistants to not only check that all cases have been covered, but
even to automatically generate the list of cases for the human prover
to consider, so that constructing a proof becomes an interactive
effort.

\LONGVERSION{(proof carrying code? want to formalize your ``domain specific'' reasoning logic and mechanize
its metatheory)}

However, formalizing such proofs can still be tedious for at least one reason,
namely giving a formal treatment of variable binding and
substitution. \cite{Altenkirch93} writes (about
formalizing a proof of normalization for System F):

\begin{quote}
When doing the formalization, I discovered that the core part of the
proof (here proving the lemmas about CR) is fairly straightforward and
only requires a good understanding of the paper version. However, in
completing the proof I observed that in certain places I had to invest
much more work than expected, e.g. proving lemmas about substitution
and weakening.
\end{quote}

This issue has spawned a large body of work aiming to relieve this
particular burden.

The logical framework LF \citep{Harper93jacm} has proven to be well-suited for representing
programming language syntax, logics and hypothetical
judgments. However, it has been less clear how to formulate induction
over LF representations in a way that is directly comparable to
induction over a first-order representation like de Bruijn indices or
names. (? is this fair? cite?)

However, the most effective approaches to this problem have
come at a fairly steep price: fairly weak logics. (elaborate)

There are a handful of approaches to logics and theories suitable for
machine checked proofs. Higher order logic, set theory, and dependent
type theory are the main alternatives which are rich enough to serve
as a foundation for modern mathematics. Among these, dependent type theory
\citep{Martin-Loef73a} boasts the advantage that it comes equipped with a direct
connection to computer programming. This means that it provides a rich
environment which at the same time supports both proofs and
programs. By constructing a proof in dependent type theory, one has
implicitly also constructed a corresponding algorithm which can be
directly executed. This is very valuable, because it
enables the development of theory to proceed hand in hand
with implementation. Moreover, (intensional) dependent type theory
incorporates automated support for a decidable equational theory in a
principled way, which is an important asset for the proposed work.
NuPRL \citep{NuPRL} Agda \citep{Norell:phd07} Coq
\citep{bertot/casteran:2004}

The main claim I aim to support with my thesis work is the following:

\begin{quote}Building a logical framework into dependent type
theory is possible and makes mechanization of state-of-the-art programming
language metatheory more effective.
\end{quote}

Moreover: the resulting framework shows promise for a rich programming
environment in which to implement programming languages and logics, which allows
implementation to be carried out jointly with a ``sliding-scale'' of
verification.

More generally, I hope that the lessons learned from these efforts can shed some light on how
to make mechanization of proofs outside of programming languages and
logic more practical. A main component of the proposed work is the
incorporation of a (particular) rich, decidable equational theory into a
proof assistant, and this effort seems very likely to offer insight into how to
incorporate other equational theories from other areas of application.


\section{Background}
Sketch well-scoped representation in Agda and substitution
principles. Illustrate the point that the main bottleneck for
mechanization are the equations which arise (e.g. church rosser proof?
similar experiments as far as proving normalization for MLTT with a universe)

\begin{itemize}
\item Standard issues in mechanizing metatheory: first order
  representations require defining weakening, substitution, and proving
  equations pertaining to (compositions of) weakening and
  substitution.
\item Higher-order abstract syntax solves some of these issues, but it
  is not so clear how to combine with rich type theories and logics
\item Contrast dependent type theory with indexed types (e.g. Beluga) and first-order
logic (e.g. Abella). 

\item Primary examples are logical relations, like Coquand's type
directed conversion (and our ITP submission), soundness/completeness of NbE
(cite e.g. Abel for recent work, Dybjer for original?),
step-indexed logical relations for reasoning about effects

\item clarify distinction with Hybrid: I'm proposing to use language design
techniques to give better support (definitional equalities for
simultaneous substitutions).

\item design space: systems with support for binding and substitution vs
systems with rich abstraction mechanisms and logical strength

Definition of substitution with names:

\begin{eqnarray*}
[N/x](x) & = & N \\
 \lbrack N/x](y) & = & y \quad \text{if $x \neq y$} \\
 \lbrack N/x](M_1 M_2) & = & ([N/x]M_1) ([N/x]M_2) \\
 \lbrack N/x](\lambda y. M) & = & \lambda y. [N/x]M \quad
 \text{choosing $y \not\in fv(N)$ and
  $y \neq x$}
\end{eqnarray*}

%``A domain-specific dependent type theory''

% though actually this isn't the worst of it, the worst of it is
% establishing necessary equational properties of substitution and weakening
% (freshness in the nominal world, shifts in the de Bruijn world) (cite
% Altenkirch) and deducing equations from the resulting equational
% theory.

%classic: $[P/y]([N/x]M) = [([P/y]N)/x] ([P/y]M)$ provided $x\neq y$
%(check this: Barendregt?)

%distinction between techniques for supporting ``names'', weakening, freshness (nominal) and ``substitution''.

%dependent type theory allows effective programming and proving
\end{itemize}

Proofs in programming language metatheory have lots of cases (which
can often be generated/developed interactively), which is why they're
a good candidate for formalization.

Existing systems targetting support for programming language
  metatheory handle well proofs which require modest logical
  strength, such as proofs of confluence and type safety. However,
  increasingly the programming language community is interested in
  topics such as normalization, parametricity \citep{Reynolds,morerecent?}, decidability of
  typechecking \citep{Coquand?}, (step-indexed) logical relations for contextual
  equivalence \citep{Dreyer11,others}, soundness and completenes of
  NbE \citep{Dybjer00,Abel07}, all of which require
  richer logics to express. Sometimes also simple requirements like
  concatenation of contexts -- can't directly be expressed in Twelf
  (can in Beluga), can often be worked around, but why not directly? 
  By giving a more seemless integration of a logical framework
  into dependent type theory (i.e. no separation between levels) this
  can be accomplished.

Also forgetful maps on contexts (v.s. representation using relations
and proving totality, etc)

Also to properly justify logical relations arguments and use existing
understanding of termination. (We ``cheat'' in Beluga to do logical relations)

(Can I claim that the PL community is one of the biggest users of
proof assistants, hence it makes sense to target?)

We give up a bit: intend to build around a de Bruijn
representation. Argue that for proofs it doesn't matter much:
typically we deal with one or two binders at at time. Scoping is
enforced by typechecking, which guides you where to insert
shifts. It actually takes quite a bit of effort to get shifts wrong. 

Pro of de Bruijn indices and contextual types (vs names, nominal
logic): purely equational; no ``propositional'' side
conditions. Compare:

$[N/x]M = M$ if $x \not\in fv(M)$, versus:

$[0 := N]M = M$ if $M = [\uparrow]M'$ for some $M'$.

This fits better into dependent type theory.

HOAS/contextual types are ``syntax'' on top of de Bruijn indices.

cite structural logical relations: requires auxiliary logic, deviates
somewhat from the on-paper techniques? \citep{Schurmann08}

cite POPLMark \cite{POPLMark}

cite Altenkirch complaining about de Bruijn indices

CoqMT approach to implementation?

In defense of intensional type theory: *smaller* checkable
witnesses. (both beta and OL substitution equations)

Spells out equality testing -- algorithm, not heuristic.

\subsection{Conversion in type theory}
A central issue in the design of a dependent type theory is
conversion: when are two types deemed to be equal? In
intensional type theories, conversion is decidable, but somewhat
weak, typically including only $\beta$ laws and perhaps some $\eta$
laws. There has been substantial work on the design and verification
of algorithms for deciding increasingly richer theories, typically
with more forms of $\eta$ laws. To take a
small selection: \cite{Coquand91,Harper05,Abel11}. Recently,
\cite{Allais13} have developed promising techniques for incorporating equations
which are neither $\beta$ nor $\eta$ laws, such as the associativity
and unit laws for list append. It remains open to establish if 
these techniques can be applied to rich dependent type theories.

\subsection{Indexed recursive types}
Recently, we have developed LF-indexed recursive types
\citep{Cave12}, as a stepping stone to support LF-indexed inductive
types. This was a first step in exploring how HOAS types can fit
into a picture with traditional recursive (or inductive..) datatypes. 

These increase the expressive power of the system: we can express
predicates on LF objects which were inexpressible before: e.g. that
certain subexpressions are closed (they do not refer to any free variables), and express relationships between
contexts. Generally, it gives direct access to properties of
contexts. This is in contrast to Abella and Delphin, and Twelf, which do not
provide such direct access to the context.

This is not ideal: typically, the relations between contexts were are
interested in expressing are \emph{functional} relations. This means
that often we are obligated to prove that they are deterministic and
total. It would be substantially more convenient to treat them
directly as functions, but this is not supported by the framework of
indexed types. 

Recursive types are suitable for programming, but they do not suffice
if we are interested in proofs: need positivity. don't quite suffice
for logical relations: need universe (large eliminations) to justify,
since they aren't positive definitions.
\subsection{First class substitutions}\label{sec:lfmtp13}
Our work on first-class substitutions \citep{Cave13} allows
abstraction over simultaneous (sometimes known as parallel)
substitutions, which is a natural technique used in many proofs
(logical relations). For example, a typical logical relations proof
takes a form like the following:

\begin{quote}
If $\Gamma \vdash M : A$ and $\sigma \in \mathcal{R}_\Gamma$ then
$[\sigma]M \in \mathcal{R}_A$.
\end{quote}

Here we have quantified over all substitutions $\sigma$ providing
instantiations for all the free variables in $M$. This is a capability
which is often not present in systems with rich support for
substitutions, since the context $\Gamma$ is often ambient instead of
available explicitly. 

In such proofs, there is typically a need to
prove equations about simultaneous substitutions, for example:
$[N/x][\sigma,x/x]M = [\sigma,N/x]M$ provided $x \not\in
fv(\sigma)$. Our work in \citep{Cave13} solves this issue by embedding
a decision procedure for these sorts of equations into the proof checker.

Our approach works well for the setting of indexed typed we develop,
but it does not scale well to dependent types.

In this work we give the first example of a logical relation proof.

\subsection{Logical relations in contextual type theory}
We have developed a larger case study of a formalization
\citep{Cave14} of a logical relations proof using the techniques of
\citep{Cave12} and \citep{Cave13}.
\subsection{Example}
\section{Remaining to be done}
\subsection{Computation in types}
My goal for the future is to integrate a logical framework into
dependent type theory. This provides an avenue to solve several of the
above mentioned issues:


1) Expressing functional relations on contexts as genuine
functions. Many other examples of ``computation in types''.

2) An avenue for analyzing termination

3) Justifying the reasonableness of logical relations (universes)

4) Reuse understanding of inductive types

This is necessary to support state-of-the-art proofs involving polymorphism,
universes, ``generic programming'', dependent types, step indexing.

basic design sketch

\begin{verbatim}
data exp (G : ctx) : tp -> Set where
 v : var G T -> exp G T
 app : exp G (arr T S) -> exp G T -> exp G S
 lam : exp (G , T) S -> exp G (arr T S)
\end{verbatim}

\begin{alltt}
[_] : sub G' G -> exp G T -> exp G' T
[_]' : sub G' G -> sub G D -> sub G' D
\end{alltt}

\begin{alltt}
lem1 : [ [ \(\sigma1\) ]' \(\sigma2\) ] = [ \(\sigma1\) ] \(\circ\) [ \(\sigma2\) ]
lem2 : [ \(\sigma\) , M ] \(\circ\) \(\uparrow\) = [ \(\sigma\) ]
lem3 : [ id ]' \(\sigma\) = \(\sigma\)
lem4 : [ id , N ] \(\circ\) [ \(\uparrow\) \(\sigma\), 1 ] = [ \(\sigma\) , N ]
...
\end{alltt}
This is tedious to do for each language we study!

We know that LF works well to encode object languages...

So do this *once* for LF, and embed your OL inside

I've prototyped this in Agda

$>$48GB of memory to typecheck an example

And we still have to apply these lemmas manually and often compose several of them

Solution: build LF datatype into DTT and build lemmas into conversion check

Result: ``dependently typed Beluga''

Analyze LF objects by the induction principle you get from considering an inductive type representing LF

provides ``simple'' approach to semantics, termination (via eliminator for LF datatype)?

The goal is (success is measured by) an algorithm for typechecking,
together with a proof of this algorithm's correctness: termination,
soundness, and completeness. The standard technique for proving this
kind of result about a dependent type theory also implies a few very
important corollaries: consistency of the theory, as well as
normalization of the calculus as a programming language.

This proof is highly non-trivial. This is ``Deep Theory''

technique for induction principle: induct over ``all of LF'' uniformly, permitting mutual definitions, etc.

technical results I am aiming to achieve: decidability
  of typechecking, (strong) normalization

Algorithm for typechecking: \citep{Coquand91,Harper05,Abel11}

Along lines of DML, ATS: specialized for metatheory. ``New equations for neutral terms'' \cite{Allais13}

The second main component is to implement a prototype sufficient to
check some non-trivial proofs. I am to formalize 

\cite{Coquand98} normalization proof for Martin-L\"of type theory with a universe

soundness and completeness of a normalization by evaluation algorithm
as presented by \cite{Dybjer00}. 

These are substantial proofs which, to the best of my knowledge, have never been
formalized in a proof assistant with sufficient support for
substitution and binding. I aim to do these proofs because I have carried out these formalizations in
Agda, with gaps in the proof pertaining to substitutions. I have found
that these proofs are quite manageable once they are understood, with
the main exception of substitutions, which, for these proofs, will be
entirely handled by support for the equational theory of
substitutions, as described in Section \ref{sec:lfmtp13}.

If it turns out that such proofs will not work for some reason, there
are alternative candidates to illustrate the point: a logical relation
for contextual equivalence (parametricity) in the style of
\cite{Dreyer11} or normalization for (perhaps a predicative variant
of) System F \citep{ProofsAndTypes,Altenkirch93}.

intensional type theory is a language of verifiable evidence
(McBride's term?)

Alternative approaches: may ask why a language design? aren't there
already tools we can use to do this? e.g. proof by reflection, CoqMT, VeriML?
\section{Timeline}
\begin{itemize}
\item Next 6 months (May 2014 to Nov 2014): working out the metatheory and examples of computation in
types. Optimistically, a POPL paper in July. ESOP deadline in October. 
\item 6 months (Nov 2014 to May 2014): prototype implementation and implementing examples

 - Implementing examples is key to demonstrating the value of this
 approach. I have conducted many such proofs in Agda: There are three
 main practical obstacles to carrying out such proofs 1) one needs a
 sufficient understanding of techniques for dealing with de Bruijn
 indices 2) one of course needs to understanding the proofs 3)
 applying the equational theory of simultaneous substitutions. 1 and 2
 I have tackled, and 3 is still a burden: this is what I propose to address
\item 6 months (May 2015 to Nov 2015): writing thesis.
\item backup/option: Some kind of ``initial algebra semantics'' for hypothetical judgments?
\item optional piece: support for (and examples of) step-indexed
  logical relations
\item (Journal version of rec types paper combined with subst. vars?)
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{proposal}
\end{document}