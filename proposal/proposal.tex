\documentclass{article}
\usepackage{natbib}
\usepackage{color}
\usepackage[pdftex, pdfborderstyle={/S/U/W 0}]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{proof}
\usepackage{listings}
\lstloadlanguages{ContextualML}
\lstset{language=ContextualML}  % Added to local lstlang2.sty

\renewcommand{\t}[1]{{\fontsize{1.0\zzlistingsize}{1.05\zzlistingsize}\texttt{#1}}}
% Set up listings "literate" keyword stuff (for \lstset below)
\newdimen\zzlistingsize
\newdimen\zzlistingsizedefault
\zzlistingsizedefault=9pt
\zzlistingsize=\zzlistingsizedefault
\global\def\InsideComment{0}
\newcommand{\Lstbasicstyle}{\fontsize{\zzlistingsize}{1.05\zzlistingsize}\ttfamily}
\newcommand{\keywordFmt}{\fontsize{0.9\zzlistingsize}{1.0\zzlistingsize}\bf}
\newcommand{\smartkeywordFmt}{\if0\InsideComment\keywordFmt\fi}
\newcommand{\commentFmt}{\def\InsideComment{1}\fontsize{0.95\zzlistingsize}{1.0\zzlistingsize}\rmfamily\slshape}

\newcommand{\LST}{\setlistingsize{\zzlistingsizedefault}}

\newlength{\zzlstwidth}
\newcommand{\setlistingsize}[1]{\zzlistingsize=#1%
\settowidth{\zzlstwidth}{{\Lstbasicstyle~}}}
\setlistingsize{\zzlistingsizedefault}
\lstset{literate={->}{{$\rightarrow~$}}2 %
                 {=>}{{$\Rightarrow~$}}2 %
                 {id}{{{\smartkeywordFmt id}}}1 % 3 $~$
                 {\\}{{$\lambda$}}1 %
                 {\\Pi}{{$\Pi$}}1 %
                 {\\Sigma}{{$\Sigma$}}1 %
                 {\\Pibox}{{$\Pibox$}}1 %
                 {\\psi}{{$\psi$}}1 %
                 {\\phi}{{$\phi$}}1 %
                 {\\sigma}{{$\sigma$}}1 %
                 {\\gamma}{{$\gamma$}}1 %
%                 {mlam}{{$\lambda^{\scriptscriptstyle\Box}$}}1 %
                 {mlam}{{$\lambda$}}1 %
                 {FN}{{$\Lambda$}}1 %
                 {<<}{\color{dGreen}}1 %
                 {>>}{\color{black}}1 %
                 {^}{{$\cdot$}}1 %
               ,
               columns=[l]fullflexible,
               basewidth=\zzlstwidth,
               basicstyle=\Lstbasicstyle,
               keywordstyle=\keywordFmt,
               identifierstyle=\relax,
%               stringstyle=\relax,
               commentstyle=\commentFmt,
               breaklines=true,
               breakatwhitespace=true,   % doesn't do anything (?!)
               mathescape=true,   % interprets $...$ in listing as math mode
%               tabsize=8,
               texcl=false}

\input{notation}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\author{Andrew Cave}
\title{Dependent type theory for contextual reasoning}

\definecolor{light-gray}{gray}{0.5}
\newcommand{\LONGVERSION}[1]{{\color{light-gray}#1}}

\begin{document}
\maketitle

\section{Introduction}

%(todo: look at other examples of thesis statements)

Increasingly, there is a desire to write computer checked proofs for
research in programming languages and logic
\citep{POPLMark}. Results in programming languages appear to be prime
candidates for computer checked proofs. For one, these proofs typically take the
form of a proof by induction with many cases, and formalizing inductive
reasoning is by now quite well understood. Moreover, the abundance of cases means that such proofs can be
tedious and error-prone to write and to verify by hand. Fortunately, it is now commonplace for proof
assistants to not only check that all cases have been covered, but
even to automatically generate the list of cases for the human prover
to consider, so that constructing a proof becomes an interactive
effort.

However, formalizing such proofs can still be tedious for at least one reason,
namely giving a formal treatment of variable binding and
substitution. \cite{Altenkirch93} writes (about
formalizing a proof of normalization for System F):

\begin{quote}
When doing the formalization, I discovered that the core part of the
proof (here proving the lemmas about CR) is fairly straightforward and
only requires a good understanding of the paper version. However, in
completing the proof I observed that in certain places I had to invest
much more work than expected, e.g. proving lemmas about substitution
and weakening.
\end{quote}

This issue has spawned a large body of work aiming to relieve this
particular burden. Nominal logic \citep{Pitts2003165} supports names and weakening
effectively, but is relatively silent about substitution. The
logical framework LF \citep{Harper93jacm} has proven to be well-suited
for representing programming language syntax, logics and hypothetical
judgments and it provides direct support for substitution using an
idea called higher-order abstract syntax (HOAS): bindings are
represented using LF's \emph{function space} while substitution
become LF application. While higher-order
abstract syntax has proven to be one of the most 
effective approaches to the problem of binding and substitution, it
has come at a fairly steep price: most theories and systems
incorporating higher order abstract syntax have fairly weak
logics. This is in part because traditionally, it was unclear how to
reconcile the traditional notions of implication and function spaces with
the ``weak function space'' used in HOAS as a purely \emph{representational} technique.

For example, Twelf \citep{Pfenning99cade} can only express $\Pi_2$
statements, i.e. statements of the form $\forall x \exists y. \psi$
where $\psi$ contains no quantifiers. Beluga \citep{Pientka:IJCAR10}, Abella \citep{Gacek:IJCAR08}, and
Delphin \citep{Poswolsky:DelphinDesc08} are
somewhat better: they are all, roughly speaking, first order logic
with induction. These weak logics suffice for many results in
programming language metatheory, such as type soundness, but it is
difficult, if not outright impossible, to express more sophisticated
results such as normalization and parametricity, or generally to build
interesting models. If we wish to mechanize these (and other
state-of-the-art) results, we need foundations which lie closer in
strength to foundations of mathematics, and we want as much support as
possible for variable binding and substitution.

There are a handful of approaches to logics and theories rich enough
to serve as foundations for mathematics. Higher order logic, set theory, and dependent
type theory are the main alternatives. Among these, dependent type theory
\citep{Martin-Loef73a} boasts the advantage that it comes equipped with a direct
connection to computer programming. This means that it provides a rich
environment which at the same time supports both proofs and
programs. By constructing a proof in dependent type theory, one has
implicitly also constructed a corresponding algorithm which can be
directly executed. This is very valuable, because it
enables the development of theory to proceed hand in hand
with implementation. Moreover, (intensional) dependent type theory
incorporates automated support for a decidable equational theory in a
principled way, which allows fully formal proofs to be remarkably
short (at the cost of a more expensive proof-checking
procedure). This is often an important practical asset. 

The main claim I aim to support with my thesis work is the following:

\begin{quote}Building a logical framework into dependent type
theory is feasible and makes mechanization of state-of-the-art programming
language metatheory more effective.
\end{quote}

Moreover, there is good reason to believe that the resulting framework
could be a rich programming
environment in which to implement programming languages and logics, which allows
implementation to be carried out jointly with a ``sliding-scale'' of
verification. \LONGVERSION{(proof carrying code? want to formalize your ``domain specific'' reasoning logic and mechanize
its metatheory)}

More generally, I hope that the lessons learned from these efforts can shed some light on how
to make mechanization of proofs outside of programming languages and
logic more practical. A main component of the proposed work is the
incorporation of a (particular) rich, decidable equational theory into a
proof assistant, and this effort seems very likely to offer insight into how to
incorporate other equational theories from other areas of
application. \LONGVERSION{Expand on this later when we talk about conversion}

\section{Background}\label{sec:background}
To make the issues involved more concrete, and later to give a sketch
of proposed work, I briefly describe the
well-known issues involved in dealing with a first-order representation of
syntax. Below I sketch an intrinsically typed encoding of the
simply typed lambda calculus in a simplified Agda-like notation \citep{Norell:phd07}. This
is a well-known technique (see e.g. \citep{Benton:12}) for
economically describing well-typed expressions, and enforcing some
invariants about them in a lightweight way.

\begin{verbatim}
data exp (G : ctx) : tp -> Set where
 v : var G T -> exp G T
 app : exp G (arr T S) -> exp G T -> exp G S
 lam : exp (G , T) S -> exp G (arr T S)
\end{verbatim}

Here, \lstinline{exp} is a family of types indexed by  a context
\lstinline{G} (which we can think of a list of types) and its
type. We have three constructors, \lstinline{v}, \lstinline{app}, and
\lstinline{lam}. Variables are represented as de Bruijn indices and
embedded into terms with \lstinline{v}, but we do
not go into detail about this here. When describing an operational
semantics, we quickly have need to describe substitution of
expressions for variables. This in itself is not so difficult,
although it is somewhat repetitive. We can
describe (simultaneous) substitutions \lstinline{sub G' G} which provide
instantiations for the variables in \lstinline{G} as expressions in
\lstinline{G'}, and write a function which applies a substitution to
an expression. We may also write a function which composes two
substitutions. We also need functions for weakening terms
\lstinline{exp G T} to \lstinline{exp (G , S)$\,$ T}, which are equally
burdensome, but which I do not discuss below.

\begin{alltt}
[_] : sub G' G -> exp G T -> exp G' T = ...
[_]' : sub G' G -> sub G D -> sub G' D = ...
\end{alltt}

This is sufficient to describe the operational semantics
of our language, but if we wish to prove results about the operational
semantics (for example, confluence), we quickly find that we need to
prove some equations about the application of substitutions. Typical
results we may need are listed below:

\begin{alltt}
compositional : [ [ \(\sigma1\) ]' \(\sigma2\) ] = [ \(\sigma1\) ] \(\circ\) [ \(\sigma2\) ]
projection    : [ \(\sigma\) , M ] \(\circ\) \(\uparrow\) = [ \(\sigma\) ]
identityLeft  : [ id ]' \(\sigma\) = \(\sigma\)
combination   : [ id , N ] \(\circ\) [ \(\uparrow\) \(\sigma\), 1 ] = [ \(\sigma\) , N ]
\end{alltt}

For example, \lstinline{lem1} says that the composition of applying
substitutions is the application of the composition of
substitutions. I have only listed four of these lemmas, and only the
ones pertaining to substitution, not to weakening. In all, one
typically finds that it is necessary to prove about a dozen such lemmas. 

One begins to lament: all this effort for one of the simplest imaginable programming
languages? As the number of constructs in our language grows, or as we
begin to consider other languages, these lemmas quickly become a major
burden. These problems can not be blamed on de Bruijn indices: if we were to use a named representation of variables, these
lemmas simply take different forms, but do not really become any
simpler.

Higher-order abstract syntax solves many of these issues. The 
idea of HOAS is to directly embed the language of study inside a
relatively weak language, commonly LF. Term formers become constants
and bindings become functions. Substitution becomes application. This
technique proves to be very effective for representing syntax,
hypothetical judgments, and obtaining substitution principles. For
example, the type of intrinsically typed expressions corresponding to
the first order representation we wrote above can be written as an LF
signature as follows: 

\begin{lstlisting}
exp  : tp -> type.
app : exp (arr A B) -> exp A -> exp B.
lam : (exp A -> exp B) -> exp (arr A B).
\end{lstlisting}

There is no case for variables, as they are
treated implicitly. We emphasize that the function space in the
\lstinline{lam} case is a weak, \emph{representational} function space
-- there is no case analysis or recursion, and hence only genuine
lambda terms can be represented. To convince oneself of this fact, one can prove that this definition is
\emph{adequate}: that there is a (compositional) bijection between normal LF terms of
this type, and our on-paper notion of terms. We do not go into details
about this here. To clarify, one cannot view this signature literally as an inductive
definition, since the apparently negative occurrence of
\lstinline{exp} prevents one from doing so.

The challenging aspect of HOAS is then how to give this definition the same
status as a first-order representation. That is, we wish to somehow
perform induction over a HOAS representation. The
common goal of many systems employing HOAS, such as Beluga \citep{Pientka:IJCAR10} and
Abella \citep{Gacek:IJCAR08} can be seen as recovering this ``first-order view'' of
apparently higher-order representations. Typically, such systems
achieve this by maintaining a strict separation between the two
levels: the specification level (e.g. LF) and the reasoning level. In many ways, this
separation, as it is implemented, is too strict to be completely practical. It is one goal of
my thesis to demonstrate that this separation can in fact
be much more liberal to achieve a more practical theory and system.

In the Beluga system, if \lstinline{M} is an LF term of type
\lstinline{exp A} in context \lstinline{$\Psi$}, we package it
together with its context, forming a \emph{contextual object} \lstinline{[$\Psi$ $\vdash$ M]} of
type \lstinline{[$\Psi$ $\vdash$ exp A]}, which we can use an index to
a computation level type in order to reason
about it. This means that one has direct access to inspect and manipulate
contexts, in contrast to Twelf and Delphin. This capability is important for mechanizing state-of-the-art
metatheory. This point will be described in more depth in the following
sections.

\section{Summary of progress to date}
I have published two papers to date in the direction of building a logical
framework into dependent type theory, and formalizing state-of-the-art
metatheory. The first
was presented at POPL'12 \citep{Cave12} , and was a first step in
exploring how HOAS can fit into a rich type theory, targeting support
for indexed recursive types. The second was presented at LFMTP'13
\citep{Cave13}, and addresses direct support for the notion of
simultaneous substitution, a concept which has been absent from most
work on HOAS, but which is nevertheless extremely useful. I am
preparing a third paper \citep{Cave14} for submission to the Journal
of Formalized Reasoning, which explains how the tools described the
previous two papers can be successfully applied in the formalization
of a substantial logical relations proof. In this
section I briefly explain the main contributions of these papers.

\subsection{Indexed recursive types}
The POPL'12 paper develops a notion of LF-indexed recursive types
\citep{Cave12}, and describes many useful applications. This work represents a
first step in exploring how HOAS types can fit
into a rich type theory. This work emphasizes ``programming'' applications
as opposed to reasoning, since it was an open problem at the time how
to support termination analysis in this setting, which is necessary to
verify that a recursive function corresponds to a genuine inductive proof. However, the techniques
developed here also contribute to understanding of how to \emph{reason} about
HOAS encodings, even if such proofs are not yet fully formal.

Indexed recursive types significantly increase the expressive power of a system: one can express
predicates on LF objects which were inexpressible before: e.g. that
certain subexpressions are closed (they do not refer to any free variables), and express relationships between
contexts. Generally, it gives direct access to properties of
contexts. This is similar to Abella, but in contrast to Delphin, and Twelf which do not
provide such direct access to the context.

In this paper, we described an intrinsically type-preseriving
normalization by evaluation algorithm. This algorithm is traditionally
difficult or impossible for systems based on HOAS, because it requires
a comparatively rich computation/reasoning layer in order to define a semantic domain for expressions.

% \subsection{Contextual LF}\label{sec:clf}
% Contextual LF extends the logical
% framework LF \citep{Harper93jacm} with the power of contextual objects
% $\Psihat. M$ of type $A[\Psi]$. $M$ denotes an object which may refer to the
% bound variables listed in $\Psihat$ and has type $A$ in the context
% $\Psi$ (see also \citep{Nanevski:ICML05}).$\Psihat$ can be obtained from the
% context $\Psi$ by simply dropping the type annotations and keeping only the
% declared variable names. We characterize only objects in $\beta\eta$ normal
% form, since these are the only meaningful objects in LF. Furthermore, we
% concentrate here on characterizing well-typed terms, but defining kinds and
% kinding rules for types is straightforward and omitted.
%     \[ \begin{array}{lrcl}
% %     \mbox{Sorts} & s & \bnfas & \ttype \mid \tkind
%       \mbox{Atomic types} & P,Q & \bnfas & \app {\const a} {\vec M} \\ % \neutral{\const{a}}{S} \mid \Size \\
% %      \mbox{Types/kinds} & A,B,K & \bnfas & \ttype \mid P \bnfalt \Pi x{:}A.B %
% %      \mid \Pi x{:}\Size.B
%       \mbox{Types} & A,B & \bnfas &  P \bnfalt \Pi x{:}A.B % \mid \Pi x{:}\Size.B
% \\
%       \mbox{Heads}  & H & \bnfas & x \bnfalt \const{c} \bnfalt
%         p[\sigma] % \mid \const a 
% \\
%       \mbox{Neutral Terms} & R & \bnfas &  H \mid \app R N \bnfalt u[\sigma]  
% \\                                   
%       \mbox{Normal Terms} & M,N & \bnfas &  R  \mid \lam{x}{M}  
% \\
%      \mbox{Substitutions} & \sigma & \bnfas & \edot \bnfalt \id_\psi \bnfalt
%      \sigma, M \bnfalt \sigma ; H \\ % \bnfalt s[\sigma]\\
%       \mbox{Contexts} & \Psi & \bnfas & \edot \bnfalt \psi \bnfalt
%       \Psi, x{:}A
% % \\
% % \mbox{Signature} & \Sigma & \bnfas & \cdot 
% %   \bnfalt \Sigma, \const a{:}K 
% %   \bnfalt \Sigma, \const c{:}A 
% % \\[1ex]
% \end{array}\]


% \begin{center}
%   \begin{tabular}{ll}
% $\Delta ; \Psi \der M \chk A$ & Normal term $M$ checks against type $A$ \\
% $\Delta ; \Psi \der R \syn A$ & Neutral term $R$ synthesizes type $A$ \\
% $\Delta ; \Psi \der \sigma \chk \Psi'$ & Substitution $\sigma$ has domain $\Psi'$ and range $\Psi$. \\
% % $\Delta ; \Psi    \der A \chk s$ & LF types and kinds are well-formed \\
%   \end{tabular}
% \end{center}

% \begin{figure}
% \[
% \begin{array}{l}
%      \mbox{Neutral Terms}\quad \fbox{$\Delta;\Psi \der R \syn A$}
%       \hfill
% \\[0.7em]
%      % \quad
%       % \infer{\Delta; \Psi \entails \const a \syn K}
%       %       {\Sigma(\const a) = K}
%       \quad
%       \infer{\Delta; \Psi \entails x \syn A}
%             {\Psi(x) = A}
% \quad
%       \infer{\Delta; \Psi \entails p[\sigma] \syn [\sigma]_\Phi A}
%             {\Delta(p) = \# A[\Phi] & 
%              \Delta; \Psi \entails \sigma \chk \Phi }
%      \\[1em] 
%       \infer{\Delta; \Psi \entails \const c \syn A}
%             {\Sigma(\const c) = A}
% \quad
%      \infer{\Delta; \Psi \entails u[\sigma] \syn [\sigma]_{\Phi} P}
%             {\Delta(u) = P[\Phi]  % & \Delta; \Psi \entails S :
%               & \Delta ; \Psi \entails \sigma \chk \Phi }
%       \\[1em]
%       \infer{\Delta; \Psi \entails R\,M \syn [M/x]_A B 
%            }{\Delta; \Psi \entails R \syn \Pi x{:}A.B &
%              \Delta; \Psi \entails M \chk A}
%       \quad
% \\[1em]
%       \mbox{Normal Terms}\quad\fbox{$\Delta;\Psi \der M \chk A$}\hfill
% \\[0.7em]
%       \infer{\Delta; \Psi \entails R \chk Q
%            }{\Delta; \Psi \entails R \syn P & P = Q }
%       \quad
%       \infer{\Delta; \Psi \entails \lam{x}{M} \chk \Pi x{:}A.B}
%             {\Delta; \Psi, x{:}A \entails M \chk B}
%       \\[1em]
%       \mbox{Substitutions}\quad\fbox{$\Delta;\Psi \der \sigma \chk
%         \Psi'$} \hfill\\[0.5em]
%       \infer{\Delta; \Psi \entails \edot \chk \edot}{}
%       \quad
%       \infer{\Delta; \Psi \entails \sigma ; H \chk \Phi,x{:}A}
%             {\Delta; \Psi \entails \sigma \chk \Phi & \Delta ; \Psi \entails H \syn B & B = [\sigma]_{\Phi}A}
%      \\[1em]
%       \infer{\Delta; \psi,\Psi \entails \id_{\psi} \chk \psi}{}
%       \quad
%       \infer{\Delta; \Psi \entails \sigma , M \chk \Phi,x{:}A}
%             {\Delta; \Psi \entails \sigma \chk \Phi & 
%               \Delta; \Psi \entails M \chk [\sigma]_{\Phi} A}
% \end{array}
% \]
% \caption{Typing for contextual LF}
% \label{fig:lftyping}
% \end{figure}

%     \[ \begin{array}{lrcl}
%  \mbox{Context schemas} & G & \bnfas & \exists \wvec{(x{:}A)} . B  \mid G + \exists \wvec{(x{:}A)} . B \\
%       \mbox{Meta Objects} & C & \bnfas & \Psihat. R \bnfalt \Psi % \bnfalt \Psihat.\sigma
% \\
%       \mbox{Meta Types} & U & \bnfas & P[\Psi] \bnfalt \#A[\Psi] % \bnfalt \Psi[\Phi]
%         \bnfalt G
% \\ 
% %      \mbox{Free variable contexts} & \Phi & \bnfas & \edot \bnfalt
% %      \Phi, X:A\\
%       \mbox{Meta substitutions} & \theta & \bnfas & \edot \bnfalt
%         \theta, C / X
% %        \bnfalt \theta, X  % BP: we need the identity to push theta through an mlam. where we do not
%                            % have the type of X available to appropriately expand meta-variables.
% \\    
%   % \theta, \Psihat.R \bnfalt \theta, \Psihat.x  \bnfalt \theta ; q\\[0.5em]
% \mbox{Meta-context} & \Delta & \bnfas & \edot \bnfalt 
%    \Delta, X{:}U
% %  \Delta, u{:}P[\Psi] \bnfalt \Delta, p{:}A[\Psi]  \bnfalt \Delta , \psi {:}W\\
%     \end{array} \]


% \begin{figure}
%     \begin{rules}
% \mbox{Meta Terms}\quad \fbox{$\Delta \der C \chk U$}
% \hfill 
% \\
% \hfill\infer{\Delta \der {\cdot} \chk G}{}
% \quad
% \infer{\Delta \der \psi \chk G
%      }{\Delta(\psi) = G}
% \\[0.5em]
% \infer{\Delta \der \Psi, x{:}A \chk G
%      }{
%        \begin{array}{l}
%        \Delta \der \Psi \chk G 
% \\        
%        \exists \wvec{(x:B')}.B \in G \quad
%        \Delta; \Psi \der \sigma \chk \wvec{(x{:}B')} \quad
%        A = [\sigma]_{\wvec{(x:B')}} B 
%        \end{array}
%      } 
% \\[1em]       
% %\mbox{Meta-Terms}\hfill \\
% \infer{\Delta \der \Psihat. \sigma \chk \Phi[\Psi]
%      }{\Delta; \Psi \der \sigma \chk \Phi}
% \quad
% \infer{\Delta \der \Psihat. R \chk P[\Psi]
%      }{\Delta; \Psi \der R \chk P}
% \quad
% \infer{\Delta \der \Psihat. x \chk \#A[\Psi]
%      }{% \Delta;\Psi \der \Psi(x) = A : \ttype
%          \Psi(x) = A}
% \\[1em]
% \infer{\Delta \der \Psihat. p[\pi] \chk \# B[\Psi] }{
%        \Delta(p) = \# A[\Phi] & 
% %       \Delta ; \Psi \der p[\sigma] \syn A &
% & 
% %       \Delta; \Psi \der A = B : \ttype
%          \Delta ; \Psi \der \pi \chk \Phi &
%          [\pi]_{\Phi}(A) = B
%        }
% %\\[0.5em]
% %\mbox{where}\;\pi\;\mbox{is a variable substitution} 
% \\[1em]
% \mbox{Meta-Substitutions} \quad \fbox{$\Delta \der \theta \chk \Delta'$}\hfill \\[0.7em]
% %\framebox[10cm]{
% \infer{\Delta \vdash \cdot \chk \cdot}{} 
% \quad
% \infer{\Delta \der \theta, C/X \chk \Delta', X{:}U
%      }{\Delta \der \theta \chk \Delta' &
%        \Delta \der C \chk \msub{\theta}_{\Delta'}(U)
%      }
%    \end{rules}
% \caption{Typing for meta-terms}
% \label{fig:metatyping}
% \end{figure}

I give below a brief overview of the results of this work, which
describes the integration of LF-indexed recursive datatypes into 

Indexed recursive types have been considered before
by e.g. \cite{Zenger:TCS97} and \cite{Xi99popl}. There are two main
novel aspects of our work:
%1) A modular description of the index
%language and the computation language allows one to consider
%alternative index languages with minimal impact on the metatheory or
%implementation of the computation language
1) the application of these techniques to an index language
supporting notions binding and substitution; i.e. an index language
(in this case, Contextual LF) rich enough to talk about object
logics. 2) The treatment of index contraints, and our treatment of
pattern matching and refinement of indices is simpler than other
known approaches. The resulting language is a prime candidate for
implementing code transformations, algorithms over abstract syntax,
and (partial) verification. This claim has since been validated by
\cite{Belanger13} with the implementation of a type preserving closure
conversion algorithm and CPS conversion.

I briefly recap the main technical aspects of this work. The
language of (computation-level) types is presented below, with the
most important type formers highlighted. I write $U$ to stand for an index
language type, $\mu$ contructs indexed recursive types, and $C_1 = C_2
\wedge T$ expresses an equational constraint on indices together with
a type. We write $\dsum{\wvec{l : T}}$ for a labelled disjoint sum of
types. The form $\pibox X U T$ quantifies over an index language type
$U$. Notice that it cannot quantify over a computation level type $T$,
and so this is not really a dependent type former, it is an
\emph{indexed} type former: one should think of it more like a
universal quantifier as in first-order logic.

\[
\begin{array}{l@{~}r@{~}c@{~}l}
% \mbox{Recursive Type} & S & \bnfas & 
%          C = C \bnfalt T 
%   \bnfalt S_1 & S_2 
%   \bnfalt S_1 + S_2  \bnfalt \exists X:U. S 
%
%  \\[2pt] 
\mbox{Kinds} & K & \bnfas & \ctype \bnfalt \pibox X U K \\

\mbox{Types} & T  & \bnfas & 
        \m{Unit} 
\bnfalt Z 
\bnfalt T_1 \arrow T_2   
\bnfalt  T_1 \times T_2 
\bnfalt \dsum{\wvec{l : T}} 
\\
& & &   
\bnfalt \pibox X U T 
\bnfalt \sigmabox X U T 
\bnfalt {\color{blue} C_1 = C_2 \andalso T}
\\
&&&    
 \bnfalt {\color{blue} \mu Z.\,\mlam {\vec X}  T}
 \bnfalt {\color{blue} T\;{\vec C} }
% \bnfalt T\;C
\bnfalt U
% \mbox{Definition} & D & \bnfas & \lambda X.D \bnfalt \bnfalt \bnfalt T
\\[2pt] %  
\mbox{Context} & \Gamma & \bnfas & \cdot \bnfalt \Gamma, Z:K \bnfalt \Gamma, x:T
\end{array}
\]

Below, I illustrate how to write a type of lists of booleans indexed by their
length, which we call vectors. The type of vectors is defined
recursively: either the index $X$ is 0 and we are at the end of the
list or there is some $Y$ such that $X$ is the successor of $Y$, then
a boolean and, recursively, a vector of indexed by $Y$.

\[
\begin{array}{r@{\;}c@{\;}l@{~:~}l}
% \nat:& \multicolumn{2}{l}{\ttype.}\\ \relax
% 0 : & \multicolumn{2}{l}{\nat.}\\\relax
% \m{s}: & \multicolumn{2}{l}{\nat \arrow \nat.}\\[1em]
%
\text{Vec} = \mu \text{Vec}. \lambda X. & \langle & \m{nil} &  X = 0 \andalso \tunit\;, \\
&  & \m{cons} &  \sigmabox Y \nat X = \m{s}\;Y \andalso \m{bool} \times \text{Vec}\;Y  \;\rangle 
\end{array}
\]

For our purposes, we are interested in using such indexed recursive
types to enforce invariants about syntax and related notions such as contexts.
For example, one often has the need to express the notion of a relation
between contexts when relating two languages. A typical example might relate a context
which lists both term variables and type variables to a corresponding
context which lists only the type variables (e.g. when representing
System F). Below we write this relation in surface syntax of Beluga and the
corresponding form in the core theory. 

% \begin{lstlisting}
% datatype ctx_rel : ctx -> cctx -> ctype
% | rnil : ctx_rel [] []
% | rsnoc : ctx_rel \psi$\;$\phi -> ctx_rel (\psi,x:tm) (\phi,x:ctm)
% \end{lstlisting}

\begin{lstlisting}
datatype Ctx_rel : tmctx -> tpctx -> ctype
| nil : Ctx_rel [] []
| tmsnoc : Ctx_rel [\psi]$\;$[\phi] -> Ctx_rel [\psi,x:tm] [\phi]
| tpsnoc : Ctx_rel [\psi]$\;$[\phi] -> Ctx_rel [\psi,a:tp] [\phi,a:tp]
\end{lstlisting}

% \[
% \begin{array}{r@{\;}c@{\;}l@{}l}
% \mu \text{Ctx\_rel}. \lambda \psi\lambda \phi. & \langle & \m{nil}\; &:  \psi
% = \cdot \andalso \phi = \cdot \andalso \tunit\;, \\
% &  & \m{snoc} & : \Sigma \psi'{:}{\m{ctx}}. \Sigma \phi'{:}{\m{cctx}}.\\ 
% &&& \psi = \psi', x{:}\m{tm} ~\andalso~ \phi = \phi', y{:}\m{ctm} \\
% &&& \andalso\;\text{Ctx\_rel}\;\psi'\;\phi' \;\rangle 
% \end{array}
% \]

Here is its corresponding representation in the core language: 

\[
\begin{array}{r@{\;}c@{\;}l@{}l}
\mu \text{Ctx\_rel}. \lambda \psi\lambda \phi. & \langle & \m{nil}\; &:  \psi
= \cdot \andalso \phi = \cdot \andalso \tunit\;, \\
&  & \m{tmsnoc} & : \Sigma \psi'{:}{\m{tmctx}}. \psi = \psi', x{:}\m{tm}
\andalso\;\text{Ctx\_rel}\;\psi'\;\phi ,\\
&  & \m{tpsnoc} & : \Sigma \psi'{:}{\m{tmctx}}. \Sigma
\phi'{:}{\m{tpctx}}. \psi = \psi', x{:}\m{tp} ~\andalso~ \phi = \phi',
y{:}\m{tp} \\
&&& \andalso\;\text{Ctx\_rel}\;\psi'\;\phi' \;\rangle 
\end{array}
\]

This is a typical example of
functional relation. We will often be obligated to prove manually
that it is in fact functional, which would be unnecessary if it
were represented directly as a function, such as the one
below. However, while the language in this paper supports writing such
functions, it does not support \emph{reasoning} about them, since they
cannot occur in indices. So such a function is of limited
utility; it cannot completely replace the corresponding relation.
\begin{eqnarray*}
|-| & : & \text{tmctx} \rightarrow \text{tpctx} \\
| \psi , x:\text{tm} | & = & | \psi | \\
| \psi , a:\text{tp} | & = & | \psi | , a:\text{tp}
\end{eqnarray*}

I present below the syntax for the computation language. The
corresponding typing rules can be found in Fig. \ref{fig:comptyping}. This
is mostly standard. The recursive types are \emph{iso-recursive}
types, so \lstinline{fold} contructs an inhabitant of such types, and
they are destructed by pattern matching. Case branches $B$ have a
\emph{refinement substitution} $\theta$ which describes how variables
of the index language are refined with more information when a
particular pattern matches. \LONGVERSION{TODO: Explain in more detail}

\[
\begin{array}{l@{~}r@{~}c@{~}l}
\mbox{Expressions} & I  & \bnfas & y 
  \bnfalt \app I E % \bnfalt \fst I \bnfalt \snd I 
  \bnfalt \app I \ctxi C 
%  \bnfalt \unfold I
   \bnfalt  (E : T)\\
\mbox{~(synth.)} & & & 
% \Bnfalt  \letpack{u}{x}{e}{e'}
\\[2pt]
\mbox{Expressions} & E & \bnfas &  I 
%  \bnfalt \boxm{\Psihat}{R} 
  \bnfalt C % \tbox\, C
  \bnfalt \fn{y}{E}
  \bnfalt \mlam{X}{E} 
  \bnfalt \rec{f}{E} 
  \bnfalt \m{unit}\\
\mbox{~(checked)}& & &   \bnfalt \fold E  \bnfalt \dsum{l=E}   \bnfalt \pack (C , E) \\
% & & &   \bnfalt \letpack {(X , y) = I} E
&&&  \bnfalt (E_1, E_2) \bnfalt \casebox{I}{\vec{B}} \\
\mbox{Pattern } & {pat} & \bnfas & x \bnfalt C \bnfalt \m{unit} \bnfalt \fold {pat} \bnfalt \dsum{l = {pat}} \\
                         &          &            & \bnfalt \pack (C , {pat}) \bnfalt ({pat}_1, {pat}_2)\\
\mbox{Branch  } & B  & \bnfas &  \casearm{\Delta ; \Gamma\;.\; pat : \theta}{E}
%%&&& 
%%                         \bnfalt \casearm{\Delta ; \Gamma\;.\;pat : \theta}{\impos} 
\\[2pt]
\mbox{Contexts} & \Gamma  & \bnfas &  \edot \bnfalt \Gamma, y{:}T \\
\end{array}
\]  

\begin{figure}\label{fig:comptyping}
  \centering
\[
\begin{array}{l}
\multicolumn{1}{l}{\fbox{$\Delta;\Gamma \der I \syn T$}\quad
\mbox{Expression $I$ synthesizes type $T$}}\hfill\\[0.8em]
\infer{\Delta; \Gamma \entails y \syn T}
      {y{:}T \in \Gamma} 
\quad
\infer{\Delta ; \Gamma \der I \syn T}{
       \Delta ; \Gamma \der I \syn C = C \andalso T
}
 \\[0.8em]
 ~~~
\infer
 {\Delta ; \Gamma \entails I\;E \syn T}{
  \Delta ; \Gamma \entails I \syn T_2 \arrow T ~~&~~
  \Delta ; \Gamma \entails E \chk T_2 }
 \\[0.8em]
 \quad
\infer
  {\Delta ; \Gamma \entails \app I \ctxi C \syn \msub{C/X}T}
  {\Delta ; \Gamma \entails I \syn \pibox X U T
    ~~&~~ 
   \Delta \entails C \chk U }
%\\[0.7em]
\quad
\infer
   {\Delta ; \Gamma \entails (E : T) \syn T}
   {\Delta ; \Gamma  \entails E \chk T }
% \quad
% \infer{\Delta ; \Gamma \der \unfold I \syn [T/Z]\msub{\vec C / \vec X}S}{
%        \Delta ; \Gamma \der I \syn T\;{\vec C} & 
%        T = \mu Z.\,\mlam {\vec X} S
% }
\\[0.7em]
\multicolumn{1}{l}{\fbox{$\Delta;\Gamma \der E \chk T$}\quad
\mbox{Expression $E$ checks against type $T$}}\\[0.8em]
\infer{\Delta ; \Gamma \der \dsum{l_i=E_i} \chk \dsum{\wvec{l:T}}}
      {\Delta ; \Gamma \der E_i \chk T_i\quad\mbox{where}~l_i:T_i \in \wvec{l:T}}
% \\[0.8em]
%\\[1em]
\quad
\infer %[\Delta = \vec X : \vec U(\vec X)]
   {\Delta; \Gamma ~\entails~  \rec{f}{E} ~\chk~ T }
   {\Delta ;  \Gamma, \, f : T
     ~\entails~ E ~\chk~ T}
\\[0.8em]
\infer
    {\Delta; \Gamma \entails  \fn{y}{E} \chk T_1 \arrow T_2}
    {\Delta; \Gamma, y{:}T_1 \entails E \chk T_2 } 
\quad
% \\[0.8em]
\infer
 {\Delta ; \Gamma \entails \mlam{X}{E} \chk \Pibox X{:}U.T }
 {\Delta, X{:}U ; \Gamma \entails E \chk T }
\\[0.8em]
%\quad
\infer{\Delta ; \Gamma \der (E_1,E_2) \chk T_1 \times T_2}{
       \Delta ; \Gamma \der E_1 \chk T_1 & 
       \Delta ; \Gamma \der E_2 \chk T_2
}
\quad
\infer
  {\Delta ; \Gamma \entails I \chk T' }
  {\Delta ; \Gamma \entails I \syn T & T = T'}
%\\[0.8em]
\\[0.8em]
% \infer{\Delta ; \Gamma \der \inl E \chk T_l + T_r}{\Delta ; \Gamma \der E \chk T_l }
% \quad
%  \infer{\Delta ; \Gamma \der \inr E \chk T_l + T_r}{\Delta ; \Gamma \der E \chk T_r}
% \\[1em]
%\\[1em]
\infer{\Delta ; \Gamma \der \pack (C , E) \chk \sigmabox X U T}{
       \Delta \der C \chk U & 
       \Delta ; \Gamma \der E \chk \msub{C/X}T
}
\quad
 \infer{\Delta; \Gamma \entails % \tbox\,
          C \chk U}
       {\Delta \entails C \chk U }
\\[0.8em]
\infer{\Delta ; \Gamma \der \fold E \chk  (\mu Z. \mlam {\vec X}  S)\;{\vec C}}{
%       T = \mu Z. \mlam {\vec X} . S& 
       \Delta ; \Gamma \der E \chk [\mu Z. \mlam {\vec X} S/Z]\msub{\vec C/\vec X}S}
%\quad
\quad
\infer{\Delta ; \Gamma \der E \chk C = C \andalso T}{
       \Delta ; \Gamma \der E \chk T
}
\\[0.8em]
% \infer{\Delta ; \Gamma \der \letpack {(X,y) = I} E \chk T}{
%        \Delta ; \Gamma \der I \syn \sigmabox X U S & 
%        \Delta, X{:}U ; \Gamma,y {:}S \der E \chk T
% }
% \\[1em]
\infer
   {\Delta; \Gamma \entails \casebox{I}{\vec B} \chk T }
   {\Delta; \Gamma \entails I \syn S % T\;\vec C
     &
     \mbox{for all}~i~ 
    \Delta; \Gamma \entails B_i  \chkbranch{S} T  %& 
%    \Delta; \Gamma \entails B_r  \chkbranch{S_r} T  %& 
%      \Delta; \Gamma \entails B_r  \chkbranch{S_r} T  
} 
\\[0.8em]
\multicolumn{1}{l}{\fbox{$\Delta ; \Gamma \der B \chkbranch{S} T$}\quad
\mbox{Branch $B$ with pattern of $S$ checks against $T$}} \\[0.8em]
\infer
  { \Delta ; \Gamma \entails \Delta_i ; \Gamma_i\,.\,{pat} : \theta_i \mapsto E \chkbranch{S} T}
  {% () \not\in {pat} &
%   \begin{array}{c}
  \Delta_i  \entails  \theta_i  \chk \Delta  & 
    \Delta_i ; \Gamma_i \der {pat} \chk \msub{\theta_i} S &
    \Delta_i ; \msub{\theta_i}\Gamma, \Gamma_i \entails E \chk  \msub{\theta_i}T  
   }
%\\[1em]
%\infer
%  { \Delta ; \Gamma \entails \Delta_i ; \Gamma_i\,.\,{pat} : \theta_i \mapsto \impos \chkbranch{S} T}
%  {() \in {pat} & \Delta_i ; \Gamma_i \der {pat} \chk \absurd \msub{\theta}S & 
%   \Delta_i  \entails  \theta_i  \chk \Delta  }
%\\[1em]
%\end{array}
%\]
%\[
%\begin{array}{l}
\end{array}
\]   
%
  \caption{Typing for computations}
  \label{fig:typcomp}
\end{figure}

The main technical result in this paper is a machine-checked proof of type
soundness for this language, which I developed in the Coq proof
assistant \citep{bertot/casteran:2004}.

This work was an important milestone in understanding how HOAS can fit
into the more general setting of an expressive type theory, and how it
can be used effectively in implementing algorithms over syntax with
sophisticated invariants enforced by the type system. However, it left open a few
questions, first how to transition from indexed recursive types to
\emph{inductive types} over LF representations, and closely related, how to support termination analysis for
such types. These steps are crucial if we wish to transition from
programming with type-enforced invariants to \emph{proving}. Another question is whether we can move
from indexed types to \emph{dependent types}, which support reasoning
about the functions we write.

For example, if \lstinline{tm} is a type family indexed by a context of type
\lstinline{tmctx} containing both term and type variables (as
described above), and \lstinline{tp} is a type family indexed by a
context of type \lstinline{tpctx} containing only type variables, one
can imagine writing a function to infer the type of a term in context
$\Gamma$ with the following type signature. We use the function $|-|$
to map from a \lstinline{tmctx} to its corresponding \lstinline{tpctx}
defined above.

\begin{lstlisting}
infer : {$\Gamma$:tmctx} [$\Gamma$ $\vdash$ tm] -> Option [ |$\Gamma$| $\vdash$ tp]
\end{lstlisting}

However, in the theory described in this paper, one must
instead write a type signature like the following, where we must
express the mapping as a relation and later prove it is functional.

\begin{lstlisting}
infer : {$\Gamma$:tmctx}{$\Delta$:tpctx} Ctx_rel [$\Gamma$] [$\Delta$] -> [$\Gamma$ $\vdash$ tm]
 -> Option [$\Delta$ $\vdash$ tp]
\end{lstlisting}

While it is true that one can typically rewrite a proof using this
relational style to avoid using such features, dependent types are a
powerful linguistic mechanism for abstraction with practical impact. 

\subsection{First-class substitutions}\label{sec:lfmtp13}
The LFMTP'13 paper \citep{Cave13} addresses direct support for
simultaneous (sometimes known as parallel)
substitutions, which is a notion that arises in many programming
language proofs. However, previously this notion had been
conspicuously absent from work on logical frameworks and HOAS. This is
likely because this notion arises most commonly in logical relations
proofs, which are often out of reach of HOAS-based systems. To
illustrate the benefit from direct support for
simultaneous substitutions, this paper describes the formalization of such
a logical relations proof, using one of the simplest
imaginable such proofs: weak (head) normalization for the
simply-typed lambda calculus. This formalization appears to be one of
the most direct and succinct formalizations of a logical relations
proof to date, which demonstrates that the framework described in the
paper shows significant promise for formalizing more sophisticated results. I briefly recap
this formalization here. 

% \begin{quote}
% If $\Gamma \vdash M : A$ and $\sigma \in \mathcal{R}_\Gamma$ then
% $[\sigma]M \in \mathcal{R}_A$.
% \end{quote}

% Here we have quantified over all substitutions $\sigma$ providing
% instantiations for all the free variables in $M$. This is a capability
% which is often not present in systems with rich support for
% substitutions, since the context $\Gamma$ is often ambient instead of
% available explicitly. 

% In such proofs, there is typically a need to
% prove equations about simultaneous substitutions, for example:
% $[N/x][\sigma,x/x]M = [\sigma,N/x]M$ provided $x \not\in
% fv(\sigma)$. Our work in \citep{Cave13} solves this issue by embedding
% a decision procedure for these sorts of equations into the proof checker.

\subsubsection{Hand written proof}

Here I go over the informal presentation of the proof to explain
the translation to a HOAS-based formalization, and to draw attention
to the aspects one often ignores in a paper proof, but which have
significant impact on a formalization.

The grammar for the calculus includes lambda abstractions, variables, application, and a single
constant $\const c$ of a constant type $\const i$. Lambda abstractions and the
constant $\const c$ are considered values.

\newcommand{\stepsto}{\longrightarrow}

\[
\begin{array}{lrcl}
\mbox{Types} & A,B & \bnfas & {\const i} \bnfalt A \arrow B \\
\mbox{Terms} & M,N & \bnfas & {\const c} \bnfalt x \bnfalt \lam x M \bnfalt M\;N \\
\mbox{Values} & V & \bnfas & {\const c} \bnfalt \lam x M
\end{array}
\]

The typing rules are standard and use the judgment $\Gamma \vdash M : A$ where
$\Gamma$ describes a typing context.

\[
\begin{array}{l}
\infer{\Gamma \vdash x : A}{\Gamma(x) = A}
\quad
\infer{\Gamma \entails \lam x M : A \arrow B}
      {\Gamma,x{:}A \der M : B}
\quad
\infer{\Gamma \entails {\const c} : {\const i}}{}
\quad
\infer{\Gamma \entails M\; N : B}
      {\Gamma \entails M : A \arrow B & \Gamma \entails N : A}
\end{array}
\]

We then define weak head reduction below. We say that a term $M$ halts if there exists a value $V$ such that $M
\stepsto^* V$. 

\[
\begin{array}{l}
\infer[\mathsf{beta}]{(\lam x M) N \stepsto [N/x]M}{}
\quad
\infer[\mathsf{app}]{M\;N \stepsto M'\;N}{M \stepsto M'}
\end{array}
\]


We can now define the notion of reducibility which is typical of such logical
relations proofs. This technique goes back to \cite{Tait67}. A term is reducible at base type precisely
when it halts. A term $M$ is reducible at arrow type $A \arrow B$ when it halts, and
for every reducible $N$ at type $A$, the application of $M$ to $N$ is
reducible at type $B$.

\[
\begin{array}{l}
\mathcal{R}_{\const i} = \{M\; |\; M \text{ halts}\} \\
\mathcal{R}_{A \arrow B} = \{M\; |\; M \text{ halts and } \forall N
\in \mathcal{R}_A, (M\; N) \in \mathcal{R}_B \}
\end{array}
\]

It is trivial from the definition that if a term $M$ is reducible at
some type $A$, then it halts. One can easily prove that
$\mathcal{R}_A$ is closed under
expansion by induction on the type:

\begin{lemma}[Closure under expansion]
If $M' \in \mathcal{R}_A$ and $M \stepsto M'$ then $M \in \mathcal{R}_A$
\end{lemma}

Our aim is to show that all well-typed closed terms are reducible, but as
usual we must generalize to showing that all closed instantiations of
a well-typed open term are reducible. For this we need the notion of a
simultaneous substitution, and to define reducibility of
substitutions. If $\sigma = M_1/x_1,M_2/x_2,...,M_n/x_n$, we say
$\sigma$ is reducible at context $\Gamma = x_1{:}A_1,...,x_n{:}A_n$ if
each $M_i$ is reducible at $A_i$, i.e. $M_i \in \mathcal{R}_{A_i}$. We
write this as $\sigma \in \mathcal{R}_\Gamma$. We can now state the main
lemma:

\begin{lemma}[Main lemma]\label{lem:main}
If $\Gamma \vdash M : A$ and $\sigma \in \mathcal{R}_\Gamma$ then
$[\sigma]M \in \mathcal{R}_A$.
\end{lemma}
\begin{proof}
By induction on the typing derivation. 
We show only the interesting case:

Case $\begin{array}{l}\infer{\Gamma \der \lam x M : A \arrow
    B}{\Gamma,x{:}A \der M : B}\end{array}$:

First, $[\sigma](\lam x M) = \lam x ([\sigma,x/x]M)$ halts, since it
is a value. Suppose then that we are given $N \in \mathcal{R}_A$.

\begin{enumerate}
\item $[\sigma,N/x]M \in \mathcal{R}_B$ (by I.H. since extending
  $\sigma$ with $N$ is reducible)
\item $[N/x][\sigma,x/x]M \in \mathcal{R}_B$ (property of
  substitution; $x$ assumed fresh for $\sigma$)
\item $(\lam x ([\sigma,x/x]M))\; N \in \mathcal{R}_B$ (by closure
  under expansion)
\end{enumerate}
Hence $[\sigma](\lam x M) \in \mathcal{R}_{A \arrow B}$ (by definition)
\end{proof}

\begin{corollary}[Weak normalization]
If $\der M : A$ then $M$ halts.
\end{corollary}
\begin{proof}
By the main lemma, we have that $[\cdot]M\in \mathcal{R}_A$
and hence that $[\cdot]M$, which is equal to $M$, halts.
\end{proof}

The first point to draw attention to is the quantification
over the context $\Gamma$ and the quantification over a simultaneous
substitution $\sigma$ providing instantiations for $\Gamma$. This
quantification typically cannot be directly expressed in HOAS-based
systems, since the context is typically ambient, so that direct
access to the context is impossible. This is the case in Twelf and
Delphin. In Abella, one has to construct the notion of simultaneous
substitutions, and manually prove properties of them.

The second point to draw attention to in this proof is the use of
properties of simultaneous substitutions in the main
lemma and its corollary. On paper, these properties look intuitive and
innocuous, but as explained earlier, these properties are typically a source of
significant overhead when formalizing such results using various first-order 
representations of variable binding. However, these
properties are immediate in our framework, as
the equational theory of (simultaneous) substitutions is built into
our type theory. This means that the formalization need only implement
the main content of the proof as it is written on paper, and not these
technical details.

\subsubsection{Formalization}\label{sec:belugaweaknorm}

Here we demonstrate the particularly elegant encoding of this proof in
the Beluga system, extended with support for first
class (simultaneous) substitutions. By hand we defined grammar and
typing separately, but here it is more convenient to define
intrinsically typed terms directly. Below, \lstinline{tm} defines our family of simply-typed lambda
terms indexed by their type as an LF signature: term formers are
represented as LF constants. In typical higher-order abstract syntax fashion, lambda
abstraction takes a \emph{function} representing the abstraction of a
term over a variable.

\begin{lstlisting}
tp  : type.               
i   :  tp.
arr : tp -> tp -> tp.

tm  : tp -> type.
app : tm (arr A B) -> tm A -> tm B.
lam : (tm A -> tm B) -> tm (arr A B).
c   : tm i.
\end{lstlisting}

We can then encode our step relation in a similar fashion. Notice in
particular we use LF's application to encode substitution in the
\lstinline{s/beta} case. We define also which terms are values, and
what it means for a term to halt (not shown). 

\begin{lstlisting}
step  : tm A -> tm A -> type.
s/beta : step (app (lam (\x. M x)) N) (M N).
s/app  : step M M' -> step (app M N) (app M' N).
\end{lstlisting}

Reducibility cannot be encoded as an LF signature, since it
involves a genuine function space. Hence we move to the
computation layer of Beluga, and employ an
indexed recursive type, as defined in the previous section. Contextual LF objects
and contexts which are embedded into computation-level types and programs are 
written inside \lstinline![  ]!. 

Once again, a term of type \lstinline{i} is reducible if it halts, and a term
\lstinline{M} of
type \lstinline{arr A B} is reducible if it halts, and moreover for every
reducible \lstinline{N} of type \lstinline{A}, the application
\lstinline{app M N} is reducible. We write \lstinline!{N:[$\vdash$ tm A]}!
for explicit $\Pi$-quantification over \lstinline{N}, a closed term of type
\lstinline!A!. To the left of the turnstile in \lstinline{[$\vdash$ tm A]} is where
one writes the context the term is defined in -- in this case, it is empty.

\begin{lstlisting}
datatype Reduce : {A:[$\vdash$ tp]} {M:[$\vdash$ tm A]} ctype =
| I   : [$\vdash$ halts M] -> Reduce [$\vdash$ i] [$\vdash$ M]
| Arr : [$\vdash$ halts M] ->
        ({N:[$\vdash$ tm A]} Reduce [$\vdash$ A] [$\vdash$ N] -> Reduce [$\vdash$ B] [$\vdash$ app M N])
        -> Reduce [$\vdash$ arr A B] [$\vdash$ M];
\end{lstlisting}

In this definition, the arrows represent the usual computational
function space, not the weak function space of LF. We note that this
is \emph{not} an inductive definition: it is not (strictly) positive, since \lstinline{Reduce}
appears to the left of an arrow in the \lstinline{Arr} case. Here we note that this
definition is, in some sense, stratified by the \lstinline{tp} index: the recursive occurences of
\lstinline{Reduce} are at types \lstinline{A} and \lstinline{B} which
are smaller than \lstinline{arr A B}. This is how one justifies its
existence: the type is, in a sense, \emph{computed} by primitive
recursion on the index argument \lstinline{tp}. Beluga does not currently perform either a positivity check
or a stratification check, so in the present we must convince ourselves that this
predicate is well-defined. In Coq \citep{bertot/casteran:2004} and Agda
\citep{Norell:phd07} one typically justifies such definitions using a
universe (large elimination), which Beluga does not currently
support. I comment more on this point later.

Next we prove closure of \lstinline{Reduce} under expansion. Proofs
by induction take the form of recursive functions. We show the
statement of the lemma, but we do not go into detail about the proof,
since it follows the handwritten proof closely.

\begin{lstlisting}
rec closed : [$\vdash$ step M M'] -> Reduce [$\vdash$ A] [$\vdash$ M'] -> Reduce [$\vdash$ A] [$\vdash$ M]
\end{lstlisting}

Now we arrive at the part of the proof requiring simultaneous
substitutions. The type of simultaneous substitutions is a new built-in
type former of the index language: we write \lstinline{g[h]} for the
type of substitutions which provide, for each variable in the context
\lstinline{g}, a term in context \lstinline{g}.

We must state precisely what it means for a substitution to be
reducible. We do this by employing another indexed recursive type: a predicate
expressing that the substitution was built up as a list of reducible
terms. This predicate is defined on substitutions of type
\lstinline{[$\vdash$ g]}, i.e. taking variables in \lstinline{g} to closed
terms of the same type. In the base case, the
empty substitution is reducible. In the \lstinline{Cons} case, we read
this as saying: if \lstinline{$\sigma$} is a reducible substitution
(implicitly at type \lstinline{[$\vdash$ g]}) and \lstinline{M} is a reducible
term at type \lstinline{A}, then \lstinline{$\sigma$} with \lstinline{M}
appended is a reducible substitution (implicitly at type
\lstinline{[$\vdash$ g,x:tm A]} -- the domain has been extended with a
variable of type \lstinline{A}).

\begin{lstlisting}
datatype RedSub : (g:ctx){$\sigma$:[$\vdash$ g]} ctype =
| Nil : RedSub [$\vdash$ ^$\,$ ]
| Cons : RedSub [$\vdash$ $\sigma$ ] -> Reduce [$\vdash$ A] [$\vdash$ M] -> RedSub [$\vdash$ $\sigma$, M ];
\end{lstlisting}

% We implicitly quantify over the context \lstinline!g! by using round
% parenthesis writing \lstinline!(g:ctx)!; we explictly quantify over substitution
% variables using curly braces writing \lstinline!{#S:g[]}!. 

Finally, our fundamental theorem is standard and takes the form we would expect
from the handwritten proof: if \lstinline{M} is a well-typed
term, and we provide a reducible substitution \lstinline{$\sigma$} with closed
instantiations for each of the free variables of \lstinline{M}, then
\lstinline{M $\sigma$} (that is, the application of \lstinline{$\sigma$} to
\lstinline{M}) is reducible. We proceed by induction on the
term. Again we show only the interesting \lstinline{lam} case. Clearly
the lambda abstraction halts, as it is a value (this is the first,
boxed argument to \lstinline{Arr} below). To show 
that applying it to any reducible term is reducible, we appeal directly to
closure under expansion, and the induction hypothesis for
\lstinline{M1} with the substitution extended with
\lstinline{N}. Recall that \lstinline{rN} is a proof that
\lstinline{N} is reducible.

\begin{lstlisting}
rec fund : {g:ctx}{M:[g $\vdash$ tm A]} RedSub [$\vdash$ $\sigma$] -> Reduce [$\vdash$ A] [$\vdash$ M $\sigma$] =
mlam g => mlam M => fn rs => case M of
| [g $\vdash$ lam (\x. M1)] =>
  Arr [$\vdash$ h/val s/refl val/lam]
   (mlam N => fn rN => closed [$\vdash$ s/beta]
    (fund [g,x] [g,x $\vdash$ M1] (Cons rs rN)))
...
\end{lstlisting}

Weak normalization is now a trivial corollary, taking \lstinline{M} to be a closed term and
\lstinline{#S} to be the empty substitution:

\begin{lstlisting}
norm : {M:[$\vdash$ tm A]} [$\vdash$ halts M] = mlam M => main (fund [] [$\vdash$ M] Nil)
\end{lstlisting}

Notably, we did not have to concern
ourselves with the property of substitutions that we wrote explicitly
in the paper proof in the \lstinline{lam} case. Using a low-level representation
of substitution (e.g. de Bruijn indices) one must prove $[\sigma,N/x]M = [N/x][\sigma,x/x]M$
(where $x \not\in \FV(\sigma)$) by hand, and this is actually a large
bulk of the proof. In this setting, it is handled automatically
by normalization during typechecking. Roughly, the
term \lstinline{fund [g,x] [g,x $\vdash$ M1] (Cons rs rN)} proves
the reducibility of \lstinline{M1[$\sigma$, N]}, while
\lstinline{closed [$\vdash$ s/beta]} is expecting a proof of the reducibility
of
\lstinline{M1[$\sigma$[$\uparrow$], 1][id, N]}. Our decision procedure for
equality of index language expressions normalizes both to
\lstinline{M[$\sigma$, N]}, so that this typechecks. In this work, the
decision procedure for equality is described using the technique of
\emph{hereditary substitution} \citep{Watkins02tr}. 


The main technical achievement of this paper is the decidability of
typechecking in the presence of such equations on substitutions, and
the implementation of these ideas as an extension to the Beluga system. The
remarkably succinct machine-checked proof in Beluga serves to
illustrate the effectiveness and utility of this type system. 

However, one unresolved issue only becomes more pressing with this work: giving a
fully formal treatment of the definition of the logical 
relation. It cannot be an inductive type, so it must be explained by
other means. This is straightforward in richer type theories/proof
assistants in which we have access to a \emph{universe}, but such
theories and systems do not provide near the same support for
substitution. Bridging this gap is one component of my proposed
work. Unfortunately, the technique of hereditary substitution appears
to not scale to such theories, so a more robust approach is needed.

\subsection{Logical relations in contextual type theory}
I am in the process of preparing for submission a larger case study of a formalization
of a logical relations proof using the techniques developed in the
POPL'12 and LFMTP'13 papers. This work is presented in the draft
\citep{Cave14}. It describes a formalization of a proof of
completeness of an algorithm for deciding equality of simply-typed
lambda terms up to $\beta\eta$ equivalence. Such a proof is the core
of the proof of decidability of typechecking for LF
\citep{Harper03tocl}, and the general technique is known to scale to
much richer type theories (e.g. \cite{Abel11}).

In this section I briefly describe the high-level structure of this
proof, focusing on the aspects which
make it challenging to formalize, and how these aspects are treated
effectively by the POPL'12 and LFMTP'13 work. The proof concerns two main judgements:

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{}}
$\Gamma \vdash M \equiv N : A$ & terms M and N are declaratively equivalent at
type $A$ \\
$\Gamma \vdash M \Leftrightarrow N : A$ & terms M and N are
algorithmically equivalent at type $A$ \\
  \end{tabular}
\end{center}

Declarative equivalence (also called definitional equality) includes convenient but non-syntax directed rules
such as transitivity and symmetry, among rules for congruence,
extensionality and $\beta$-contraction. In particular, it may include
apparently type-directed rules such as extensionality at unit type:

$$
\infer{\Gamma \vdash M \equiv N : \mathrm{Unit}}{\Gamma \vdash M : \mathrm{Unit} &
  \Gamma \vdash N : \mathrm{Unit}}
$$

Such a rule motivates the use of algorithmic type-directed
equivalence, since a more naive ``normalize-and-compare''
strategy to decide equivalence will not be able to incorporate such a
rule, since it relies on type information. Algorithmic equivalence on the
other hand is directly implementable, since it is directed by the syntax of
types and terms. 

In this paper, we formalize the proof of completeness of algorithmic equivalence for declarative
equivalence in the Beluga system. As one might expect, this proof goes
through an intermediate logical relation, which we write:

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{}}
$\Gamma \vdash M \approx N : A$ & terms M and N are logically equivalent at
type $A$ \\
  \end{tabular}
\end{center}

This relation is defined by induction on the structure of the
type. The key case is at function type, which is defined on paper as
the following:

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{~~~}l@{}}
$\Gamma \vdash M_1 \approx M_2 : A \Rightarrow B$ & iff & for all $\Delta \geq \Gamma$
and $N_1$, $N_2$, \\
& & if $\Delta \vdash N_1 \approx N_2 : A$\\
& & then $\Delta \vdash M_1\; N_1 \approx M_2\; N_2 : B$
  \end{tabular}
\end{center}

The quantification over extensions $\Delta$ of $\Gamma$ is essential
for establishing a monotonicity lemma. This Kripke-style monotonicity
is one of the reasons that this proof is more challenging than normalization proofs, where this
quantification can be avoided using other technical tricks. For
our formalization, we take a slightly different approach which 
better exploits the features available to us. We
instead quantify over an arbitrary $\Delta$ together with a
\emph{renaming} substitution $\pi$ which brings terms from $\Gamma$ to
$\Delta$, i.e. $\Delta \vdash \pi : \Gamma$. This is a substitution of
variables for variables, i.e. it embodies precisely the structural
rules of exchange, contraction, and most importantly for this proof, weakening.

\begin{center}
  \begin{tabular}{@{}l@{~~~}l@{~~~}l@{}}
$\Gamma \vdash M_1 \approx M_2 : A \Rightarrow B$ & iff & for all
$\Delta$, $N_1$, $N_2$, and renamings $\pi$ \\
& & if $\Delta \vdash N_1 \approx N_2 : A$ and $\Delta \vdash \pi : \Gamma$ \\
& & then $\Delta \vdash M_1[\pi]\; N_1 \approx M_2[\pi]\; N_2 : B$
  \end{tabular}
\end{center}

As one can imagine, the introduction of more substitutions means that
the equations which arise in the proofs become even more complex, and
the desire to discharge them automatically becomes even more
pressing. For example, the most complex equation which arises in this
proof is the following (written in de Bruijn form):

$$
M[\sigma[\uparrow],1][\pi[\uparrow],1][\text{id}, N] = M[\sigma[\pi],N]
$$

Fortunately, the LFMTP'13 work dutifully discharges this equation, and the user
does not even encounter the complex expression on the left. The formal
proof establishes the main results with essentially no bureaucratic
manipulation of substitutions:

\begin{theorem}[Fundamental theorem]
If $\Gamma \vdash M \equiv N : A$ \\ and $\Delta \vdash \sigma_1 \approx
\sigma_2 : \Gamma$ then $\Delta \vdash M[\sigma_1] \approx
N[\sigma_2] : A$
\end{theorem}

\begin{corollary}[Completeness]
If $\Gamma \vdash M \equiv N : A$ then $\Gamma \vdash M
\Leftrightarrow N : A$
\end{corollary}

This paper marks the first time such a proof has been formalized with
such minimal bureaucracy about substitutions; it appears to directly
follow the paper proof, once one has adapted slightly the notion of
monotonicity to fit the framework. This is significant evidence that
our methods can scale to modern, sophisticated proofs in programming
language metatheory.

\section{Proposed work : From Indexed to Dependent Type Theory}\label{sec:proposed}
As argued in the previous sections, dependent types are a convenient
feature for developing programming language metatheory. Moreover,
concepts such as universes (which are often the device used to justify
the definition of logical relations) are fairly well-understood in the setting of
dependent type theory, but they are far less understood in
theories with powerful support for substitution. My work to date
has demonstrated how to incorporate the equational theory of
simultaneous substitutions into indexed type theory, and its
applications to logical relations proofs, but it remains to extend
such a theory to truly dependent types. My work to date represents
important steps in establishing that such a project is feasible at
all. In this section I give a sketch of the design plan, as well as
describe the example proofs I intend to use as case studies.

\subsection{A sketch of the design plan}\label{sec:proposedtheory}

The primary value of LF lies in its ability to faithfully represent a wide
variety of object languages and hypothetical judgments by
parameterizing it by a signature. One can attempt to
gain the virtues of LF in dependent type theory by defining a
first-order (say de Bruijn) encoding of LF as an inductive type, in
much the same way as the intrinsically typed encoding in 
Sec. \ref{sec:background}, and encoding languages of interest
inside of it.

However, there are two approaches to introducing new types
into type theory. The first is to use a general mechanism for defining
(inductive) types within the system e.g. Coq's \verb|Inductive|, Agda's
\verb|data|, Beluga's \verb|datatype|. This is the technique employed by
users of the system, since these mechanisms are designed to not
introduce inconsistency to the system. The second, more radical approach, is to extend the syntax of the
language with new type formers, introduction forms, and elimination
forms. This is the language designer's technique. This must be treated
much more carefully, since for one, it can easily compromise the consistency of
the theory. However, the practical payoff can be much higher: the language
can provide more sophisticated support for the type. This is
particularly so in intensional type theory: built-in type
formers come with \emph{definitional equalities} which are verified as
part of typechecking, and users need not concern themselves with
them. Typically, these are $\beta$ rules (computation rules), and some
$\eta$ rules (extensionality rules). But one can consider also other rules,
such as the associativity of addition. To preserve decidability of
type checking, the combined set of such rules must be a
\emph{decidable} equational theory.

I have carried out a construction of LF using the first approach (as
an instance of an inductive type) in Agda, to test the feasibility of
this idea. The construction works, but requires some very experimental
features (so-called ``inductive-inductive'' definitions), and attempting to use it quickly
runs into practical limitations: it required gigantic amounts of
memory. This does not appear to be an essential problem with the
idea, but more of a deficiency in Agda's implementation. This
construction is similar in some regards to the approach taken in
Hybrid \citep{Felty12}.

I propose that the next step is to perform a construction of the
second kind, at the language design level. The equational theory of
substitutions can then be integrated into the \emph{definitional
 equality}, in much the same way as as associativity of addition
might. In some sense, this is not so
different from the language-based approach to support
substitution as found in Beluga, and this perspective appears to
provide a reasonably clear
picture of how to move from indexed types to dependent types.

By approaching the problem from this angle, I hope to use the
extensive techniques developed for dependent type theory to aid in
explaining induction over syntax with binding, universes, and
termination.

The main technical challenge lies in designing an algorithm for the
resulting definitional equality \LONGVERSION{when we move to
  dep. theory, notion of definitional equality merges equational
  substitution theory with computations; previously just ``syntactic''}, and proving its correctness
(completeness, soundness, and termination). This in fact requires fairly deep theory
 and sophisticated techniques, since for interesting equational
 theories, proving this result typically entails also proving
 consistency of the theory. The known techniques for this problem also
 prove consistency and normalization as
 byproducts. \LONGVERSION{Expand: merge with next paragraph?}.

% One objection to this design is that the resulting theory
% will be quite large, complex and specialized. My main defense is
% that my work on developing logical relations with these tools
% provides evidence that such a system would have significant
% practical value for the programming languages community. It is a more
% long term goal of mine to understand how the resulting theory can be
% understood more deeply and generally. In the meantime, I will settle
% for practical value.

\paragraph{Conversion in type theory} There has been substantial work on the design and verification
of algorithms for deciding increasingly richer theories, typically
with more forms of $\eta$ laws. To take a
small selection: \cite{Coquand91,Harper05,Abel11}. Recently,
\cite{Allais13} have developed promising techniques for incorporating equations
which are neither $\beta$ nor $\eta$ laws, such as the associativity
and unit laws for list append. It remains open to establish if 
these techniques can be applied to a dependent type theory with a
universe. But it looks promising, because the approach by
\cite{Abel07} to verify a conversion test for type theory with a
universe, and the technique by \cite{Allais13} are both based on
normalization by evaluation.

% \LONGVERSION{
% Pro of de Bruijn indices and contextual types (vs names, nominal
% logic): purely equational; no ``propositional'' side
% conditions. Compare:

% $[N/x]M = M$ if $x \not\in fv(M)$, versus: $[\text{id},N][\uparrow]M = M$

% This fits better into dependent type theory.

% HOAS/contextual types can be seen as ``syntactic sugar'' on top of de
% Bruijn indices. An explanation of how
% higher-order abstract syntax can actually be seen as first-order.

% cite structural logical relations: requires auxiliary logic, deviates
% somewhat from the on-paper techniques? \citep{Schurmann08}
% }

% \LONGVERSION{
% % The ``higher-order'' aspect of HOAS is a bit of a red herring, and
% % in some sense obstructs our understanding of how to do ``standard''
% % mathematical analysis of such encodings.

% % Analyze LF objects by the induction principle you get from considering an inductive type representing LF

% % We give up a bit: intend to build around a de Bruijn
% % representation. This is a bit ugly. Argue that for proofs it doesn't matter much:
% % typically we deal with one or two binders at at time. Scoping is
% % enforced by typechecking, which guides you where to insert
% % shifts. It actually takes quite a bit of effort to get shifts wrong. 

% Alternative approaches: may ask why a language design? aren't there
% already tools we can use to do this? e.g. proof by reflection, CoqMT,
% VeriML?
% }

\subsection{Prototype and case studies}\label{sec:prototype}
The next step is the development of a prototype implementation of this
theory, with the goal of formalizing one or two non-trivial proofs
which make use of the additional expressive power. 
The most practical and promising direction for the implementation
appears to be to adapt the existing Beluga implementation. This is for
several reasons:

\begin{itemize}
\item The Beluga implementation already implements several practical
features for proof development, namely implicit arguments, elaboration, and pattern
matching which would be burdensome to implement from scratch, and
unwieldy to live without.
\item Many of the requirements are already implemented: LF, and
the computation language.
\item I have already carried out a first step in
adapting the implementation from indexed
types to dependent types: merging the context of the index language
(LF) and the context of the computation language. This was a sweeping
change to the implementation that touched many components, but it turned
out to be fairly straightforward, which suggests that this is a
promising direction to carry out the rest of the
implementation.
\item I now have experience with the Beluga
code base, which makes adapting Beluga a more viable option than 
adapting e.g. the Agda implementation, which I am unfamiliar with.
\end{itemize}

To support the claim that the theory makes mechanization of
state-of-the-art programming language metatheory more effective, the
next step is to formalize interesting results which make use of the
additional expressive power. I have two promising candidates for these examples:

\begin{itemize}
\item A normalization proof for Martin-L\"of type theory with
  a universe as presented by \cite{Coquand98}.
\item Soundness and completeness of a normalization by evaluation algorithm
as presented by \cite{Dybjer00}. 
\end{itemize}

I have a several reasons to prefer these proofs and to believe they
are feasible to formalize in such a system:

\begin{itemize}
\item The additional power of dependent type theory (relative to indexed
types) really appears to be essential for these proofs. The second,
for example, is reasoning about a computation-level function,
 which is not possible in the world of indexed types.
\item I have partially formalized both in Agda, leaving lemmas related
  to substitutions unproved. This means that I have already carried
  out the first important step: understanding these proofs in sufficient
  detail to formalize them. I have found that both proofs are quite
  manageable once they are understood, with the main exception of
  substitutions, which become quite unwieldy.
\item It was clear from this effort that both proofs rely
  extensively on the equational theory of substitutions, and so would
  benefit substantially from the support for substitutions I aim to provide.
\end{itemize} 

These are substantial proofs which have not previously been
formalized in a proof assistant with sufficient support for
substitution and binding. There are alternative candidates for such
proofs, which will likely be left to future work, as I have not yet
gone through the effort of understanding them in sufficient detail to
formalize them: a logical relation
for contextual equivalence in the style of
\cite{Dreyer11} and normalization for (perhaps a predicative variant
of) System F \citep{ProofsAndTypes,Altenkirch93}.

If I can successfully formalize one these proofs using the tools from
this theory, it will be substantial evidence that such a theory really
can have a positive impact on the mechanization of state-of-the-art
programming language metatheory.

% \section{Related Work}
% ...

\section{Timeline}
In this section, I describe an estimated timeline for the completion
of the goals described in Sec. \ref{sec:proposed} and the
completion of my thesis.

\paragraph{Stage 1:  May 2014 to Nov 2014} Working out the
theory as described in Subsec. \ref{sec:proposedtheory}. Optimistically, I may have preliminary results to submit
as POPL paper in July. More realistically, I should target the ESOP deadline in October.

\paragraph{Stage 2: Nov 2014 to May 2015} Implementing a prototype and
example formalizations as described in Subsec. \ref{sec:prototype}.

% CoqMT approach to implementation?
\paragraph{Stage 3: May 2015 to Nov 2015} Writing thesis, for initial submission by
December 2015. This will compile my work on indexed recursive types, first class substitution
variables, and the results of my proposed work on the transition from indexed types
to dependent types.

Simultaneously, during stages 1 and 2, I aim to prepare a submission to the TOPLAS journal to
describe in more depth the theory and implementation of indexed
recursive types and first-class substitutions from 
the POPL'12 and LFMTP'13 work, and to elaborate the draft on algorithmic
equality for a submission to the Journal of Formalized Reasoning.

% backup/option: Some kind of ``initial algebra semantics'' for hypothetical judgments?
% optional piece: support for (and examples of) step-indexed logical relations
\section{Conclusion}
Together, my work to date on foundations for mechanizing metatheory
and the work I propose on bridging the worlds of dependent type theory
and logical frameworks will be a substantial contribution to
understanding the mechanization of programming language metatheory,
and shows promise for making mechanization of state-of-the-art
metatheory more effective.

\bibliographystyle{plainnat}
\bibliography{proposal}
\end{document}