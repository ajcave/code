\documentclass{article}
\usepackage{natbib}
\author{Andrew Cave}
\title{Dependent type theory for contextual reasoning}
\begin{document}
\maketitle

\section{Introduction}
Thesis statement: Building a logical framework into dependent type
theory is possible and makes mechanization of state-of-the-art programming
language metatheory more effective. (Moreover: it's a rich programming
environment. %Dependent type theory gives a ``sliding scale'' from programming to verification
)
%(todo: look at other examples of thesis statements)

Dependent type theory provides a rich environment for both proofs and programs. 

\section{Background}
Sketch well-scoped representation in Agda and substitution
principles. Illustrate the point that the main bottleneck for
mechanization are the equations which arise (e.g. church rosser proof?
similar experiments as far as proving normalization for MLTT with a universe)

\begin{itemize}
\item Standard issues in mechanizing metatheory: first order
  representations require defining weakening, substitution, and proving
  equations pertaining to (compositions of) weakening and
  substitution.
\item Higher-order abstract syntax solves some of these issues, but it
  is not so clear how to combine with rich type theories and logics
\item Contrast dependent type theory with indexed types (e.g. Beluga) and first-order
logic (e.g. Abella). 

\item Primary examples are logical relations, like Coquand's type
directed conversion (and our ITP submission), soundness/completeness of NbE
(cite e.g. Abel for recent work, Dybjer for original?),
step-indexed logical relations for reasoning about effects

\item clarify distinction with Hybrid: I'm proposing to use language design
techniques to give better support (definitional equalities for
simultaneous substitutions).

\item design space: systems with support for binding and substitution vs
systems with rich abstraction mechanisms and logical strength


%``A domain-specific dependent type theory''

% though actually this isn't the worst of it, the worst of it is
% establishing necessary equational properties of substitution and weakening
% (freshness in the nominal world, shifts in the de Bruijn world) (cite
% Altenkirch) and deducing equations from the resulting equational
% theory.

%classic: $[P/y]([N/x]M) = [([P/y]N)/x] ([P/y]M)$ provided $x\neq y$
%(check this: Barendregt?)

%distinction between techniques for supporting ``names'', weakening, freshness (nominal) and ``substitution''.

%dependent type theory allows effective programming and proving
\end{itemize}

Proofs in programming language metatheory have lots of cases (which
can often be generated/developed interactively), which is why they're
a good candidate for formalization.

Existing systems targetting support for programming language
  metatheory handle well proofs which require modest logical
  strength, such as proofs of confluence and type safety. However,
  increasingly the programming language community is interested in
  topics such as normalization, parametricity, decidability of
  typechecking, (step-indexed) logical relations for contextual
  equivalence, soundness and completenes of NbE, all of which require
  richer logics to express. Sometimes also simple requirements like
  concatenation of contexts -- can't directly be expressed in Twelf or
  Beluga, can often be worked around, but why not directly? Twelf and
  Beluga can express concatenation of lists, but contexts have special status.
  By giving a more seemless integration of a logical framework
  into dependent type theory (i.e. no separation between levels) this
  can be accomplished.

Also forgetful maps on contexts (v.s. representation using relations
and proving totality, etc)

Also to properly justify logical relations arguments and use existing
understanding of termination. (We ``cheat'' in Beluga to do logical relations)

(Can I claim that the PL community is one of the biggest users of
proof assistants, hence it makes sense to target?)

We give up a bit: intend to build around a de Bruijn
representation. Argue that for proofs it typically doesn't matter:
typically we deal with one or two binders at at time.

CoqMT approach to implementation?

\subsection{Indexed recursive types}

don't quite suffice for proofs: need positivity. don't quite suffice
for logical relations: need universe (large eliminations) to justify,
since they aren't positive definitions
\subsection{First class substitutions}

\subsection{Example}
\section{Remaining to be done}
\subsection{Computation in types}
\begin{itemize}
\item motivation
\begin{itemize}
\item richer tools (functions which compute instead of functional
relations) for more effective proofs
\item necessary to support state-of-the-art proofs involving polymorphism,
universes, ``generic programming'', dependent types, step indexing.
\end{itemize}
\item basic design sketch
\begin{itemize}
\item technique for induction principle: induct over ``all of LF'' uniformly, permitting mutual definitions, etc.
\end{itemize}
\item technical results I am aiming to achieve: decidability
  of typechecking, (strong) normalization
\end{itemize}

Along lines of DML, ATS: specialized for metatheory. ``New equations for neutral terms'' \cite{Allais13}

Logical step-indexed logical relations: \cite{Dreyer11}.

intensional type theory is a language of verifiable evidence (McBride's term?)
\section{Timeline}
\begin{itemize}
\item Next 6 months: working out the metatheory and examples of computation in
types, potentially for a POPL paper in July.
\item 6 months: prototype implementation and implementing examples
\item 6 months: writing thesis
\item backup/option: Some kind of ``initial algebra semantics'' for hypothetical judgments?
\item optional piece: support for (and examples of) step-indexed
  logical relations
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{proposal}
\end{document}